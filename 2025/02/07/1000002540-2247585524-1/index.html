<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="50000 字纯对话 | 美国AI科学家x半导体专家，唠唠DeepSeek和未来的“赌国运”之战, ZejunCao&#39;Blogs">
    <meta name="description" content="50000 字纯对话 | 美国AI科学家x半导体专家，唠唠DeepSeek和未来的“赌国运”之战

仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接

编译|王启隆出品丨AI科技大本营（ID：rgznai100）原文丨htt">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>50000 字纯对话 | 美国AI科学家x半导体专家，唠唠DeepSeek和未来的“赌国运”之战 | ZejunCao&#39;Blogs</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">ZejunCao&#39;Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">ZejunCao&#39;Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/frontcover/1000002540_2247585524_1.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">50000 字纯对话 | 美国AI科学家x半导体专家，唠唠DeepSeek和未来的“赌国运”之战</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/">
                                <span class="chip bg-color">开源项目</span>
                            </a>
                        
                            <a href="/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E8%81%9A%E5%90%88%E5%B9%B3%E5%8F%B0/">
                                <span class="chip bg-color">微信公众号聚合平台</span>
                            </a>
                        
                            <a href="/tags/AI%E7%A7%91%E6%8A%80%E5%A4%A7%E6%9C%AC%E8%90%A5/">
                                <span class="chip bg-color">AI科技大本营</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-02-07
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/TZo8Cfyp0EpHjc65jeLmKQ">50000 字纯对话 | 美国AI科学家x半导体专家，唠唠DeepSeek和未来的“赌国运”之战</a></p>
<blockquote>
<p>仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接</p>
</blockquote>
<p>编译|王启隆<br>出品丨AI科技大本营（ID：rgznai100）<br>原文丨https :&#x2F;&#x2F;lexfridman.com&#x2F;deepseek-dylan-patel-nathan-lambert<br>为了深入剖析DeepSeek带来的技术突破、市场震荡以及更深远的行业影响，最近知名人工智能播客主持人（现在已经基本是播客界顶流一哥了）LexFridman邀请了两位在AI及半导体领域具有权威见解的专家——SemiAnalysis创始人DylanPatel和艾伦人工智能研究所研究科学家NathanLambert，展开了一场五小时的深度对话。<br>这五小时一唠下来，转写成文字就是十万字的长度，微信公众号根本放不下。哪怕把一些懂的都懂的政治敏感内容删掉（这俩哥们涉及键政的内容起码有两小时，还没聊出啥爆点，都是车轱辘话），再剔除一些口语性表述（俗称就是废话）还是很多字。<br>所以我们最终筛出了其中和技术相关性比较强、聊的点比较有趣的部分，精炼语言与表达，才有了这篇长文。<br>预览几句精彩观点：<br>「DeepSeek之所以能超越Meta、Mistral、Cohere等公司，在于后者行动不够迅速，或者过于谨慎，不敢“豪赌”一把。你可以说是运气，但这也是实力的比拼。」<br>「现在孤注一掷的风险远低于2022年OpenAI那种“倾家荡产”式的“豪赌”，也比DeepSeek现在这样的“背水一战”风险要小。纵观人类历史，最大的赢家往往是那些在某个时刻敢于“赌”一把，冒险一搏的人。」<br>「在中国，平台型SaaS并不流行，大家倾向于自建技术栈，因为中国的软件工程成本低得多，STEM毕业生数量庞大等等。自建更划算。同时，代码大语言模型在中国的普及率远低于美国，因为中国工程师的成本更低。」<br>「也许AGI能解决全球变暖之类的问题，但至少现在实验室里的人们的态度是：“用天然气就用天然气吧，这场竞赛太重要了，我输不起。”」<br>「在能力层面，我们或许已经接近某种“AGI时刻”。但实际应用这些能力的成本极其高昂，以至于目前无法大规模部署，从而立刻颠覆经济格局。AGI的影响不会瞬间爆发，而是一个逐步渗透的过程。」<br>「在互连带宽上，H20与H100相当，而在内存带宽和容量上，给中国特供的H20甚至优于H100。」<br>「“智能体”这个词显然被过度炒作了。很多软件公司为了蹭热度，也把自己的产品描述成“智能体”，但这些产品本质上是一个封闭系统，在后台通过AI将你的信息应用与照片等数据整合。」<br>咱们就不塞太多字在前面了，赶紧切入正文——此外，结尾有惊喜，请务必拉到最后：<br>DeepSeek技术原理<br>LexFridman：请先介绍一下DeepSeekV3和R1分别是什么，让我们从宏观层面了解，再深入细节。</p>
<p>DeepSeekV3发布于去年的12月26日。几周后的1月20日，推理模型DeepSeekR1发布，引发了更多讨论。<br>LexFridman：我们先从“开放权重”的概念开始探讨。<br>NathanLambert：开放权重指大语言模型的权重可供互联网用户下载，并附带不同许可证，规定模型使用条款。Llama、DeepSeek、Qwen、Mistral等流行的开放权重模型都有各自的许可证，条款复杂且不尽相同。<br>关于“什么使模型成为开放权重模型”的讨论由来已久。自ChatGPT出现后，这一问题更受关注。开放权重听起来像开源，但并非完全相同。开源AI的定义和精神内核仍在争论中。传统开源软件强调修改自由、自主使用自由和使用方式不受限。这些原则在AI领域的具体体现尚待定义。<br>我当前在艾伦人工智能研究所（AI2）工作，这是一个非营利组织，致力于推动AI的开放使用，引领真正的开源。我们认为真正的开源应包括发布训练数据、训练代码和开放权重——但社区对此尚未完全达成共识。<br>至于DeepSeek的模型细节，我们稍后讨论。在了解模型训练过程时，需要强调：数据处理、过滤和质量是模型质量的首要决定因素。训练代码决定训练时长和实验速度。若数据不开放，模型复制难度将大大增加，即使公开了成本数据（如GPU时长和费用）和代码也一样。<br>LexFridman：DeepSeek模型可能是前沿模型中较开放的。理想的开源应包括开放代码、数据和权重。DeepSeek虽然不是完全开源（代码和数据可能不开放），但开放了权重，采用MIT许可证，相对自由，这在开源运动中是积极的。<br>NathanLambert：是的，DeepSeek在传播AI知识方面做得很好，其论文详细介绍了工作原理，对其他团队改进训练技术很有帮助。DeepSeekR1模型采用MIT许可证，非常宽松，商业用途和使用场景不受限制，甚至可以用模型输出创建合成数据。<br>最接近的同类模型是Llama，它也开放权重和技术报告。Llama3的技术报告非常受欢迎，但可操作性稍逊，训练细节和图表较少。Llama3的许可证也比MIT许可证更严格。<br>LexFridman：DeepSeek的影响之一是推动Llama和其他模型，包括OpenAI，走向更开放。开源的另一方面是细节公开程度，例如代码透明度和技术报告质量。DeepSeek在这方面做得不错，公开了很多细节。<br>NathanLambert：DeepSeekV3预训练论文明确指出，他们在技术栈的多个层面进行了干预，例如修改NVIDIA（英伟达）芯片CUDA层或更底层以实现高效训练。<br>LexFridman：回到更基础的问题，DeepSeekV3和R1的区别是什么？能否解释一下潜在的混淆点？<br>NathanLambert：很多人对这种模型命名感到困惑，但可以这样理解：大语言模型训练包含预训练阶段，即预测互联网文本的下一个token。DeepSeek对海量互联网文本进行预训练，得到DeepSeekV3Base基础模型，这种基础模型只能补全句子，不如ChatGPT好用。<br>之后，DeepSeek对V3Base进行两种不同的后训练，赋予模型特定行为。一种是指令微调，即对齐模型或聊天模型，这是更常见的后训练方式，产生了DeepSeekV3指令模型，性能出色，可与GPT-4等媲美。另一种是新的推理训练，产生了DeepSeekR1推理模型。R1的“R”代表推理（Reasoning）。命名方式类似OpenAI的O1推理模型。R1的训练更新颖，论文有详细介绍，是AI社区快速发展的研究领域。<br>LexFridman：我们还需要介绍预训练和后训练这两大类别。什么是预训练？后训练有哪些类型？<br>NathanLambert：预训练就是自回归预测，预测文档序列的下一个token，在数万亿token的海量数据上进行，数据主要来自网络抓取，例如CommonCrawl。预训练的关键在于高效利用计算资源，例如浮点运算（FLOPS）和GPU时长，构建高效系统，得到基础模型。预训练的复杂性在于训练损失函数的选择。<br>后训练方面，最经典的技术是指令微调(IFT)或监督微调(SFT)，即让模型学习理解指令和问题格式，并以信息丰富且易于理解的方式回应，例如回答“解释罗马帝国历史”这类问题。<br>另一类是偏好微调，包括基于人类反馈的强化学习(RLHF)。RLHF被认为是ChatGPT突破的关键技术，它使模型输出更符合人类阅读习惯，更像Reddit回答。偏好微调通过收集人类对答案好坏的偏好数据进行训练。目前AI也在参与标注数据。偏好微调使用对比损失函数，让模型学习人类偏好。具体实现方式多样，例如奖励模型和直接对齐算法。<br>最后，强化学习(RL)是较新的技术，R1和推理模型就采用了这种方法。OpenAI秋季也推出了强化微调API。强化学习是AI的一个分支，通常指试错学习，在可能嘈杂的环境中进行连续决策。大语言模型推理中，模型生成答案后，会检查答案是否正确。对于数学或代码问题，有明确的正确答案。通过多次尝试和验证，模型在可验证领域能显著提升性能。强化学习应用于大语言模型微调是近年兴起的技术，已被前沿实验室使用多年，DeepSeek的R1模型也采用了这种方法。<br>LexFridman：技术栈的各个层面都有激动人心的进展，尤其是在后训练方面，今年可能会有很多有趣的发展。<br>再回到更大众层面的问题。DeepSeekV3和R1在用户体验上有何区别？抛开技术细节，对普通用户而言，它们的使用体验和用例有何不同？各自擅长什么？<br>NathanLambert：DeepSeekV3的用户体验更常见。你提问后，模型会快速生成token，答案如同高质量的Reddit或StackOverflow回答，通常带有markdown列表，格式清晰，突出重点。V3在各领域表现出色，即使是专业知识边缘的问题，也能提供不错的答案，可以作为学习辅助工具。<br>DeepSeekR1推理模型则不同，它生成token时，会展现一个思维链过程。模型会先解释问题，分解步骤，例如“他们问我这个问题，我需要这样做……”，这些思考过程都会由模型生成并展示。R1的API速度很快，大量token迅速涌现，如同模型的推理过程实时展现在屏幕上。最终，R1会总结推理过程，给出类似V3的最终答案。DeepSeekR1的独特之处在于，即使非AI专家也能看到大语言模型如何分解问题，进行推理。<br>技术上，R1模型经过专门训练，包含推理和回答两个阶段。模型会生成一个特殊token（可能对用户隐藏），表示“开始回答”。OpenAI等模型也在尝试展示推理过程，例如分步骤显示“分解问题、进行计算、清理结果”等。<br>LexFridman：能否举例说明DeepSeekR1的推理过程？<br>NathanLambert：这张截图是推特网友的分享。<br>图上显示的是DeepSeek聊天应用的界面，顶部显示“思考了157秒”，点击下拉箭头可以查看推理过程。<br>LexFridman：他问的问题是“我倾向于哲学思考，请你提供关于人类的全新见解”。<br>R1在回答中揭示了推理过程，不断反思“这个见解是否足够新颖”，通过自我挑战变得更独特、反直觉。例如，R1推理“人类有独特的元情绪，对自身情绪有情绪，如对生气感到内疚。这种递归情绪层次创造了其他动物没有的复杂动机”。它推理人类的情绪感受和元情绪，推理过程很长，如同意识流。<br>R1还会反思“用户可能想要新颖的见解，让我深入挖掘”，并思考人类持有矛盾信念的能力，即认知失调，并推测其功能可能是为了灵活适应。<br>最终，经过157秒思考，R1给出的答案是“人类通过集体假装抽象规则、金钱、法律、权利是真实的，本能地将自私欲望转化为合作系统。这些共同幻觉如同‘游戏’，将竞争转化为社会燃料。”非常深刻，可以用这个答案专门发一条推特了。<br>DeepSeek低训练成本<br>LexFridman：如果要说DeepSeek最令人瞩目的成就，低训练成本绝对算一个，尤其是考虑到其“思维链”技术的出色表现——但我们稍后再谈思维链。我想先了解一下，DeepSeek如何在训练和应用上都做到如此低的成本？先从训练这一层开始吧。</p>
<p>这两项技术都至关重要。MoE模型在学术界已存在多年，OpenAI的GPT-4是首个将其产品化的模型。目前大家能接触到的大部分开源模型，比如Llama，都是稠密模型。这意味着Llama在生成每个Token时，所有参数（或称神经元）都会被激活。<br>而MoE模型则不同。我们可以将MoE类比于人脑的工作方式：执行视觉任务时，视觉皮层活跃；思考其他事情时，其他脑区活跃；感到恐惧时，杏仁核活跃。大脑不同区域各司其职。<br>MoE模型在一定程度上模拟了这种机制，当然复杂程度远不及人脑。MoE模型的特点是，每次运行时，只有部分组件被激活。模型预设了若干“专家(Experts)”，运行时只激活其中一部分。这大大降低了训练和应用成本。<br>然后，可以将模型参数总量视为训练过程中压缩的“知识空间(EmbeddingSpace)”。当数据输入模型时，无需每次激活所有参数，只需激活一部分。模型会学习将不同任务分配给不同专家。这是一项重大创新，它允许我们不断扩充参数的“知识空间”。<br>DeepSeek的模型参数量约为6千亿，而Llama370B和405B分别只有700亿和4050亿参数。理论上，DeepSeek模型拥有更大的“知识空间”来存储信息，甚至能压缩整个互联网的知识。但同时，每次运行时，它只激活约370亿参数。因此，训练或推理时，只需计算370亿参数，远低于Llama的700亿或4050亿。可见，MoE架构在训练和应用上都能显著降低计算成本。<br>LexFridman：我们是否应该深入分析MoE模型的实际应用，并探讨Transformer架构？<br>NathanLambert：Transformer架构这两年已被广泛讨论，所以细节方面不再赘述。本质上，Transformer是基于重复的注意力机制(AttentionMechanism)模块和传统全连接层&#x2F;多层感知器模块堆叠而成。这些模块交替使用，还有一些其他细节。<br>MoE技术就应用于Transformer的全连接层中。在Transformer模型中，全连接层占据了绝大部分权重。因此，MoE模型能显著提升参数效率和推理效率，因为效率的提升来自于运行时无需激活所有参数。<br>LexFridman：Transformer本身就是一个庞大的神经网络。过去十五年，深度学习经历了“深度学习革命”，神经网络变得越来越大。期间，规模定律(ScalingLaws)逐渐显现，即在模型规模的多个维度上，模型越大，性能越好。<br>NathanLambert：不同类型的模型都有各自的规模定律。规模定律实际上是指，对于相同的计算投入，不同架构在测试任务上的性能水平不同。MoE模型就是其中之一。在训练时，即使不考虑应用优势（实际上应用优势也很大），如果MoE架构实现得当，也能显著提高GPU效率。实际上，用大约30%的计算量就能获得性能相当的模型。当然，具体效果会因实施细节等因素而异。但重要的是认识到，这种技术创新能带来巨大收益。我预计大多数提供模型应用的公司都会转向MoE模型。<br>历史上，MoE模型可能因为实现复杂性，尤其是在处理大型模型时，没有被广泛采用。DeepSeek在这方面做得非常出色，值得称赞。DeepSeek训练基础设施的这一部分其实并非专为MoE模型而设。Dylan提到的MLA也是如此。MLA的核心思想是，通过巧妙的低秩近似数学方法，减少应用过程中的内存占用，并在训练过程中进行类似优化。<br>深入了解MLA的细节后，你会发现DeepSeek进行了非常复杂的实现。因为大语言模型还有其他部分，例如用于扩展上下文长度的嵌入(Embeddings)。DeepSeek常用的旋转位置嵌入(RoPE)，如果想将RoPE与普通的MoE一起使用，过程相对简单。你需要取两个注意力矩阵，然后通过复数值旋转（矩阵乘法）来旋转它们。而对于DeepSeek的MLA这种新的注意力架构，他们需要进行巧妙的处理，因为它们的设置方式不同，这大大增加了实现的复杂性。DeepSeek能够驾驭所有这些复杂性，这可能也是OpenAI等闭源实验室正在做的事情。我们不确定他们是否采用了完全相同的技术，但DeepSeek将这些大语言模型训练的前沿技术公开分享了，这是重点。<br>LexFridman：这些技术中的一部分需要非常底层的工程，非常繁琐，还需要各种技巧。据我所知，DeepSeek深入到了CUDA之下，对GPU进行了非常底层的编程。<br>DylanPatel：实际上，NVIDIA构建了一个通信集合库(NVIDIACommunicationsCollectivesLibrary)。训练模型时，需要在多层感知器或前馈网络与注意力机制之间进行all-reduce和all-gather操作。这是网络中所有GPU之间的通信，无论是在训练还是应用过程中。提供了一个标准库NCCL。这也是难以使用其他厂商硬件进行训练的原因之一，因为没有其他厂商构建标准的通信库。NVIDIA在更高层次上完成了这项工作。<br>DeepSeek由于可用的GPU数量有限，互连也受到一定限制，他们必须设法提高效率。其中一项措施是，他们没有直接调用NVIDIA的NCCL库，而是创建了自己的通信调度。一些实验室也会这样做。<br>Meta曾在Llama的论文中提到，他们制作了自己定制版本的NCCL，但没有透露具体实现细节。DeepSeek可能也做了类似的事情，甚至可能做得更好。因为“需求是发明之母”，DeepSeek受环境所迫必须这样做。相比之下，OpenAI、Anthropic等公司或许也有人在做类似工作，但DeepSeek显然公开了他们的做法。他们甚至可能做得更出色，因为他们在可使用的芯片方面受到一些限制。<br>因此，DeepSeek通过调度特定的SM(StreamingMultiprocessor，流式多处理器)来调度通信——可以将SM理解为GPU上的核心。GPU上有数百个甚至更多这样的核心。DeepSeek专门调度“哪些核心在运行模型？哪些核心在进行all-reduce？哪些核心在进行all-gather？”等事项，他们会在这些核心之间切换任务。这需要非常底层的编程。通常，这些调度工作是由NCCL或其他NVIDIA库自动完成的。<br>NathanLambert：从技术上讲，DeepSeek使用的是PTX，可以将其视为一种汇编语言。虽然不完全是汇编语言或指令集，但类似于直接编写汇编指令集。它仍然是CUDA的一部分。这就像在考虑，“是使用Python和PyTorch等价物，然后调用NVIDIA库？还是深入到C语言级别？或者编写更底层的代码？甚至深入到汇编或ISA级别？”<br>在大型实验室中，有时会深入到那个级别，但大多数公司不会这样做，因为这会浪费时间，且效率提升可能不值得付出。但DeepSeek的实现非常复杂，特别是他们的MoE模型。<br>此前，MoE模型已被广泛使用，但通常大家是用8个或16个专家，然后激活其中的2个。我们常用“稀疏因子(SparsityFactor)”或“使用率”来描述。模型可能只有四分之一被激活，例如Mistral的模型，他们的模型最早就是因此声名鹊起，让人们惊叹“这家法国公司真的很厉害”。OpenAI和其他主要的闭源实验室也使用MoE模型。但DeepSeek所做的，也许只有最领先的实验室才刚刚开始尝试，是采用如此高的稀疏因子。不是每次运行模型时激活8个专家中的2个（四分之一），而是激活256个专家中的8个。<br>MoE模型有不同的实现方式。可以设置一些始终激活的专家，它们类似于小型神经网络，所有Token都会通过这些专家，然后Token也会通过一些由路由机制选择的专家。DeepSeek架构的一项创新是他们改变了MoE模型中的路由机制。<br>在MoE模型中，存在一种称为“辅助损失(AuxiliaryLoss)”的机制，它实际上是为了在训练过程中确保所有专家都被用于模型所看到的任务。MoE模型可能会失败的原因是，在训练过程中，唯一的优化目标是Token预测的准确性。如果让MoE模型自行训练，它可能会学会只使用一部分专家。MoE文献中有一种辅助损失机制，可以帮助平衡专家之间的使用。但如果从深度学习的损失函数角度考虑，这甚至与AI领域经典文献“苦涩的教训(TheBitterLesson)”相关，即我们希望在模型中加入尽可能少的先验假设，以便模型最大程度地自主学习。而这种辅助损失，这种跨专家的平衡机制，可能与Token的预测准确性相冲突。<br>我们尚不清楚DeepSeekMoE具体的改动程度，即他们是否完全放弃了辅助损失，而是在路由中增加了一个额外的参数。在批处理(Batch)之后，他们更新这个参数，以确保接下来的批次使用类似的专家。这种改动可能影响巨大，也可能微小，但它们会随着时间推移而累积。这表明DeepSeek在不断创新。<br>我相信所有训练大型MoE模型的实验室都在关注这类问题，即摆脱辅助损失。他们中的一些可能已经在使用类似技术，而DeepSeek只是在不断积累优势。我们将讨论训练的哲学以及如何组织这些机构。很多时候，成功来自于随着时间的推移，在数据、架构和后期训练方面不断积累小的改进，以及这些改进如何相互整合。DeepSeek也遵循同样的策略，其中一些细节是公开的，或者大部分是公开的。我们必须相信他们分享了最重要的细节。架构和权重都是公开的，我们看到了他们的做法，这些优势都在不断累积。<br>DylanPatel：回到效率和复杂性的讨论上。Mistral和其他已公开MoE模型的稀疏比例约为4:32。而DeepSeek的比例可能更高，例如8:256或16:512。这个比例非常高。Nathan刚才提到，当稀疏度水平非常不同时，不能让每个GPU都拥有整个模型。模型太大，太复杂。<br>因此，必须使用不同类型的并行性来分割模型。可能在不同的GPU节点上部署不同的专家。但这时可能会出现一个问题：如果一批数据“看起来都一样，应该路由到模型的同一部分”，当所有数据都路由到模型的一部分时，可能会导致某一组GPU资源过载，而其余训练网络则处于闲置状态，因为所有Token都只路由到那里。<br>这是运行高度稀疏的MoE模型（例如32:4比例）的最大复杂性之一，最终会导致大量专家处于闲置状态。如何进行负载均衡？如何调度它们之间的通信？这些是DeepSeek首先公开解决的许多非常底层、非常细致的工作，并且可能是世界上第二个或第三个，甚至在某些情况下是第一个解决这些问题的团队。<br>LexFridman：从所有这些讨论中，从“苦涩的教训”的角度来看，你得出了什么结论？未来的发展方向是这种低层次的优化，能带来大量收益吗？还是说这只是短期现象，最大的收益将更多地来自算法、高层次的方面，比如后期训练？DeepSeek的飞跃是由于他们发现了一个技巧，因为受限于资源，“需求是发明之母”，从而实现了短期突破？还是说低层次优化仍然有很大的潜力？<br>NathanLambert：我认为我们应该先总结一下“苦涩的教训”的含义。“苦涩的教训”本质上可以解释为：在深度学习发展过程中，胜出的训练方法是那些在学习和搜索方面具有可扩展性的方法，这是它强调的核心。“规模”这个词因这篇文章备受关注。<br>我的理解是，“苦涩的教训”强调的是有效避免在学习过程中加入人类的先验假设。如果你阅读原文，会发现文章讨论的是研究人员如何试图为特定问题提出巧妙的解决方案，这些方案可能在短期内带来小幅收益，但从长远来看，更有效的方法是简单地使深度学习系统高效运转，并服务于更宏大的目标，这样可能更具扩展性，并持续推动成功。<br>我们现在讨论的是对MoE模型进行相对较小的实现更改。我们需要更多时间来判断这些改动是否真的与“苦涩的教训”的核心思想相关。但“苦涩的教训”实际上是从长远角度来看待简单性如何经常胜出的。业内有一种说法，模型只想学习。你只需要为模型提供一个简单的损失空间，投入计算资源，模型就能自主学习。<br>LexFridman：这就是像NCCL这样的标准库的强大之处，标准化的代码可以被很多人用来创造可以扩展的简单创新。由于用了这么多优化技巧，我想DeepSeek的代码库可能非常混乱。<br>NathanLambert：我相信DeepSeek肯定有非常混乱的代码库，他们测试了各种新想法。MLA可能最初只是在JupyterNotebook之类的环境中，或者有人在几个GPU上尝试一些东西，过程可能非常混乱。但是，用于训练DeepSeekV3和DeepSeekR1的代码库，如果展示给我们看，我猜测代码质量会非常高，可读性强——<br>LexFridman：所以是高质量、可读的代码。<br>DylanPatel：我认为需要注意一个方面，即代码的通用性，它在不同类型的运行中迁移的能力。你可能为特定大小的模型架构编写了非常高质量的代码。但当对架构进行调整时，代码就无法迁移。这可能是由于他们特定的底层代码，例如SM调度，是针对特定模型架构和大小优化的。<br>然而，NVIDIA的集合库更通用，它不在乎你的模型架构是什么，都能工作。在很多情况下，当你使用通用库时，会牺牲一些性能。但考虑到DeepSeek在计算资源方面的限制，为特定运行进行特定优化是值得的。<br>LexFridman：我想知道，训练这些前沿模型，启动训练，编写代码，然后按下“运行”按钮，这需要承受多大的压力。因为现在要花费大量金钱和时间来训练模型。我的意思是，调试阶段肯定需要大量的创新，例如，确保没有问题，需要监控和可视化训练的各个方面等等。<br>DylanPatel：当人们进行训练时，他们会使用各种仪表盘，但最基本的是损失(Loss)曲线。理想情况下，损失应该持续下降。但在实际情况中，特别是对于MoE这种更复杂的模型，以及FP8训练（另一种创新，即转向低精度数字格式）而言，会遇到“损失峰值(LossSpikes)”。没有人完全知道损失峰值出现的原因。<br>NathanLambert：对于某些情况，我们是知道原因的。有些损失峰值是由坏数据引起的。<br>我可以举AI2遭遇的一件实际案例来说明，是什么导致我们早期的模型崩溃。<br>有一个叫MicrowaveGang（微波炉帮）的Reddit版块。里面的所有用户通常只做两件事：分享自己的微波炉，或者模仿微波炉运作时的声音，也就是一串很长的“Mmm”。<br>但如果你将这种数据输入到一个训练用于生成正常文本的模型中，损失会非常高（大模型训练数据被这种玩梗的帖子污染了）。<br>DylanPatel：这让我也想到一件趣事。就是和在这些AI实验室工作的朋友一起吃饭的时候，他们会每隔10分钟就看一次手机，不是在发短信，只是在确认——“损失有没有飙升？损失有没有飙升？”<br>NathanLambert：他们会一直盯着这些指标。<br>LexFridman：哈哈，如果出现峰值，他们的心率应该也会跟着上升。<br>DylanPatel：一定程度的峰值是正常的，损失会回落。以前的策略是停止运行，从旧版本重新开始，然后更改数据组合，再继续训练。<br>NathanLambert：甚至有不同类型的峰值。在AI2，我们有一个理论，区分快速峰值和缓慢峰值。有时，观察损失和其他参数，可以看到损失开始上升，然后突然爆发，这种情况很难恢复，必须回溯到更早的版本。所以会经历一段压力很大的时期，损失曲线平稳，或者开始上升，这时你会想“我该怎么办？”<br>然而，也有些损失峰值看起来是可以接受的，只是出现一个尖锐的数据点。对于这种情况，你可以选择跳过这些数据点。看到峰值时，你可以说“好吧，我可以忽略这个数据，不更新模型，然后继续处理下一个数据，损失很快就会恢复。”<br>但是，在更复杂的实现中，当架构变得更复杂，扩展到更多GPU时，损失爆炸的可能性就越大。因此，损失峰值也呈现出分布特征。<br>DylanPatel：“整体学习(Grokking)”的概念也值得一提。仅仅因为损失不再下降，并不意味着模型没有在学习。因为突然之间，损失曲线可能会停滞一段时间，然后可能会再次下降，因为它学习了，真正地学到了一些东西。学习需要时间，这不是一个循序渐进的过程，无论是人类还是模型的学习都是如此。正如你所说，训练大型模型确实是一项压力巨大的任务。<br>LexFridman：而且整个过程中，经费也在不断流逝。<br>NathanLambert：每家公司都有失败的训练实验。你需要失败的实验来探索基础设施的极限。所以经常有新闻报道“X公司经历了Y次失败的实验”。每家试图推动AI前沿的公司都会遇到这种情况。这是值得注意的，因为失败的实验意味着大量的资金投入付诸东流，并可能造成一周甚至一个月的挫折，但这都是过程的一部分。<br>LexFridman：但DeepSeek究竟是如何找到合适的BatchSize（在深度学习模型训练过程中，每次迭代所使用的样本数量）的？<br>NathanLambert：靠的是大量小规模的失败尝试。通过快速迭代失败的实验，最终找到成功的配置。这个过程也帮助他们建立起对模型架构的直觉，例如验证了混合专家模型和MLA的有效性。<br>对于BatchSize这样的关键参数，以及学习率和正则化等，都需要根据代码库特点进行调整。我与一些前沿实验室交流过，他们认为训练大语言模型就像是探索一条既定路径，一旦掌握了训练特定类型或规模模型的能力，代码库和关于有效BatchSize的经验也就基本确定了。DeepSeek的论文和模型展现出的规模和复杂度提升，正是他们不断积累已有能力的结果。</p>
<p>它在AI领域的意思是，先在小规模上进行所有实验，研究不同的模型设置。经过无数轮尝试之后，突然有一天，你说“好了，伙计们，不要再折腾了，大家拿出所有资源，选择我们认为可行的方案，然后开始YOLO一把！”<br>由于模型规模扩展存在诸多不确定性，这种全力投入的训练尝试就被称为“YOLO实验”，带有一定的冒险意味。<br>有人认为，优秀的AI研究员应具备系统性思维，能够穷尽所有可能的参数组合，找到最优解。但也有研究员更依赖直觉，他们会说：“这次就YOLO一把，根据现有数据，我感觉方向对了。”<br>NathanLambert：因此，大家都想参与成本较低的后期训练，这样就能更频繁地进行YOLO，尝试更多可能性。<br>LexFridman：从根本上来说，就是拼运气。<br>DylanPatel：在很多情况下，运气就是实力。<br>NathanLambert：没错。即使评估结果不理想，顶尖实验室也有一套迭代改进流程。总能找到可以优化的地方，比如改进数据。持续优化累积起来，模型性能自然会提升。仔细分析模型，总能发现不足之处，然后着手改进。持续迭代是关键。所以，有些成功看似是运气，但在实践中，尤其是在我们讨论的这些新型推理模型领域，存在大量探索空间，其中一些探索往往能带来显著的性能提升。<br>DylanPatel：AI模型的搜索空间近乎无限，但我们拥有的计算资源和时间却非常有限。各家公司必须争分夺秒，赶上发布时间表，避免被竞争对手超越。DeepSeek之所以能超越Meta、Mistral、Cohere等公司，部分原因可能在于后者行动不够迅速，或者过于谨慎，不敢“YOLO一把”，也可能只是技术实力稍逊一筹。你可以说这是运气，但最终还是技能的比拼。<br>LexFridman：所以2025年将是“YOLO实验之年”。看起来所有的实验室都在全力以赴。<br>DylanPatel：OpenAI在2022年的举动更令人印象深刻。当时，MoE模型还未被广泛认可。Google拥有庞大的研究团队，而OpenAI的计算资源相对有限。但OpenAI毅然决然地将所有资源投入到GPT-4的研发上。GPT-4采用了全新的架构，能否成功当时还是未知数，但OpenAI仍然决定“投入数亿美元，赌上全部身家”。这才是真正的YOLO精神。现在大家谈论的训练失败，心态已经平和许多，因为即使一次YOLO实验失败了，大部分GPU仍然在提供应用服务，持续产生收益，同时还有大量GPU用于持续研究。<br>所以，现在的YOLO实验，风险远低于2022年OpenAI那种“倾家荡产”式的豪赌，也比DeepSeek现在这种“背水一战”式的YOLO实验风险要小。<br>LexFridman：纵观人类历史，最大的赢家往往是那些在某个时刻敢于YOLO一把，冒险一搏的人。<br>DeepSeek计算集群<br>LexFridman：关于DeepSeek的训练硬件，我们掌握多少信息？<br>DylanPatel：DeepSeek的背景很有意思。其母公司幻方是一家量化基金，长期在中国及其他地区进行交易，积累了大量GPU资源。虽然过去高频交易员用FPGA，但现在GPU是主流。DeepSeek隶属于幻方，共享母公司和CEO，因此能利用集团的交易资源和基础设施训练模型，包括大语言模型。<br>量化交易领域很早就意识到AI的价值，特别是NLP技术，能快速理解新闻等信息辅助交易。DeepSeek在此领域表现出色。过去几年，DeepSeek一方面继续运营盈利的量化基金，一方面加大AI投入。其CEO梁文锋，据称持有公司过半股份，是一位类似马斯克、黄仁勋的人物，事必躬亲，深入研究AI技术。<br>NathanLambert：他的一些访谈很有意思。我了解到，DeepSeek是他实现AGI愿景的途径。<br>LexFridman：这位CEO确实很厉害，工程师出身，全身心投入AI，还将高频交易的成功经验用于AI发展。<br>NathanLambert：他明确表示不会转向闭源。他对AI生态有长远规划，希望由中国公司来实现这愿景。<br>DylanPatel：梁文锋是DeepSeek的“远见者”。DeepSeek在论文中称V3预训练仅用2000块H800GPU，这解释了DeepSeek为何需要复杂SM调度。但预训练之后的阶段呢？2000块H800显然不是DeepSeek使用的GPU总量。<br>LexFridman：GPU总量不详。我们先聊聊预训练阶段的事情，2000块GPU用于V3训练的数据可信吗？<br>DylanPatel：需要看“训练运行”的定义，是否包含所有研究和参数实验？即使他们靠“YOLO实验”豪赌了一把，大规模训练前也必须有中小规模测试。<br>NathanLambert：业内共识是，任何有突破的模型，实验阶段的计算量通常是最终完整训练的2-4倍。<br>LexFridman：所以，DeepSeek扩大的算力，主要用于研究？<br>DylanPatel：是的，研究能带来新思路，大幅提升效率。<br>NathanLambert：研究是突破关键，必须投入。<br>LexFridman：他们的定价策略是否考虑了研究成本？<br>DylanPatel：DeepSeek公开数据只说了V3预训练的2000块GPU。R1的成本、指令模型和其他强化学习成本都没提。尽管如此，我们（Dylan隶属于半导体独立研究与分析公司SemiAnalysis）分析认为，DeepSeek实际GPU数量可能接近5万块，用于基金运营、研究和实验等任务。<br>NathanLambert：对比来看，OpenAI或Anthropic的GPU规模如何？Meta倒是很透明，他们曾透露训练集群有约6-10万块H100等效GPU。<br>DylanPatel：是的，Llama3用1.6万块H100训练。Meta去年公开购买了超40万块GPU。当然，只有部分GPU用于模型训练，大部分支持InstagramReels等应用。<br>LexFridman：接下来可以谈谈NVIDIAHopperGPU架构，以及H100和H800的区别。<br>DylanPatel：Ampere架构对应A100，之后是Hopper架构的H100。业界经常互换使用这两个名称，因为目前主要是H100，以及更新的H200，它们架构相似。<br>最初，众所周知的这个GPU出口限制其实基于两个技术指标：芯片互连速度和浮点运算能力（FLOPs）。任何芯片只要这两项指标超过一定阈值，就会受到管制。后来，意识到最初的限制存在可规避之处，又简化了规则，主要限制浮点运算能力。<br>NathanLambert：所以H800的特点是浮点运算能力强，但通信能力弱？<br>DylanPatel：正是如此。H800的浮点运算能力与H100相当，但互连带宽被削减了。DeepSeek似乎很清楚如何利用这一点，即“即使互连受限，我们也能通过技术手段充分发挥GPU性能。”<br>值得一提的是，H800禁售后，NVIDIA推出了H20芯片。H20主要降低了浮点运算能力，但互连带宽保持不变。实际上，在某些方面H20甚至优于H100，例如拥有更高的内存带宽和容量。</p>
<p>Dylan对此有深入研究。我们可以分析模型训练和推理的计算量比例。这些推理模型使得推理在复杂任务中变得更重要。例如OpenAI发布的o3模型，其突破性成果之一是在ARC-AGI任务上的表现，这是一个抽象推理的基准测试。OpenAI的o3在API中使用了一定数量的样本来解决这个任务，每个问题的成本在5到20美元，这需要大量的计算。<br>如果这类应用普及，OpenAI需要庞大的GPU资源用于推理。他们的ChatGPTPro订阅服务，每月20美元——<br>DylanPatel：SamAltman曾说这项服务在亏损。<br>NathanLambert：这意味着OpenAI用户在推理上消耗了大量GPU资源。其实o3的另一主要成就是其编程性能。如果这项技术能反哺AI公司，将有助于他们更好地进行技术实验和迭代。<br>AGI实现的时间线<br>LexFridman：可以进一步推测，对于未来的AGI（通用人工智能），更大比例的计算将用于测试时计算，用于推理。例如，AGI可能会独自走进一个小房间，思考该如何征服人类，然后三小时之后走出房间——这个过程就需要巨大的算力。<br>NathanLambert：这就是OpenAI和Anthropic等公司的领导者所说的自主AI模型，你赋予AI任务，它们在后台自主执行。<br>我认为我对AGI的个人定义更务实。我认为大语言模型本身就是一种AGI，更强大的AI是下一步。大语言模型在很多领域都展现出巨大价值，对我来说它就是一种通用智能。<br>这些AI公司未来几年的目标是，发展更具自主性、能执行训练数据之外任务的AI系统。AI将加速所有计算科学领域的进步。<br>LexFridman：我们正好接着这个话题继续聊，预测一下AGI可能出现的时间。Anthropic创始人DarioAmodei预测2026年会出现超级人工智能，具备智能体能力，构成真正的安全威胁，达到AGI的程度。你们预测的时间是什么？<br>NathanLambert：我不喜欢预测具体能力和时间，因为这非常困难。“感受到AGI”更多是基于对未来几年持续、快速、惊人进步的预期。DeepSeekR1的出现并不令我惊讶，因为我预料到新的范式会出现，并带来实质性进展。<br>DeepSeekR1令人不安之处在于，我们似乎一直在沿着ChatGPT的路径前进，它持续改进，然后我们又开辟新方向，模型性能阶梯式跃升。这种进步速度令人震惊，而且预计会持续。我试用过Claude，尝试让它执行电脑操作，但尚未达到那种程度。我理解Dario的想法，但难以预测实现通用智能所需的突破类型。更有可能的是，突破会出现在我们意想不到的领域——我认为会有更多类似R1的突破涌现。<br>LexFridman：我还是想让你给出一个AGI时间线上的具体年份，你认为何时会达到那个临界点？对我来说，可能在2030年之后，所以我的预期不像Dario那么早——<br>NathanLambert：我也是这么认为的，2030年之后。<br>DylanPatel：我们需要先定义“AGI”的标准。对我而言，AGI的影响几乎已经显现，比如有一则新闻显示，AI虚构的电话让某国选民误以为是政治家打电话过来拉选票。当今世界时事已经在各方面体现了AGI潜在影响的规模，大语言模型降低了生成高智能语言的成本。<br>NathanLambert：但有研究表明，信息传播渠道才是瓶颈。大语言模型尚未显著改变虚假信息的传播态势。互联网仍在发展。普林斯顿大学的朋友写过一篇博客，探讨了这个问题。研究表明，虚假信息并未因大语言模型而显著恶化。至少在互联网帖子等可量化指标上，没有出现指数级增长或明显变化。你提到的语音电话等形式可能更难量化。现在下结论还为时过早，许多研究人员正在监测局势。<br>总之，如果让我给出一个AGI实现的具体年份，我会参考AI公司CEO们的预测，并基于他们的预测年份再推迟几年，这就是为什么2030年或之后会成为一个合理的估计。<br>DylanPatel：我认为在能力层面，我们或许已经接近某种“AGI时刻”，任何人都可以说“如果能在X时间内利用这些能力，那就是AGI了，比如2027年或2028年。”但实际应用这些能力的成本才是关键。当前的成本极其高昂，以至于目前无法大规模部署，从而立刻颠覆经济格局。AGI的影响不会瞬间爆发，而是一个逐步渗透的过程。<br>NathanLambert：存在物理限制。<br>DylanPatel：以2023年微软必应整合GPT-4为例，当时人们对搜索引擎的变革感到震惊，Perplexity也随之兴起。但如果计算将GPT-3应用于每次Google搜索的成本，就会发现这在经济上是不可行的。回到成本问题，用ChatGPT最强大的模型回答一个问题，单次查询成本约为几美分。然而，解决一个AGI级别的问题，单次任务成本可能高达5到20美元。<br>NathanLambert：而且只会更高。<br>DylanPatel：成本相差1000到10000倍，一个是回答问题，一个是执行任务。AGI的任务复杂度更高，成本也更高。即使我们现在拥有“AGI”，三年后技术更先进，AGI可以解决更复杂的问题，但成本仍将是数千、数万甚至数十万美元的GPU时间。电力、GPU和基础设施都无法支撑AGI的瞬间普及，世界变革也将是渐进的。更重要的是，未来谁来控制和引导AGI执行任务？<br>LexFridman：说到控制AGI，我观察到，人们对机器人技术的直觉似乎存在偏差，普遍过于乐观。自动驾驶汽车就是一个例子，人们认为它比实际更易实现。无人机领域也类似，目前人类驾驶员仍然远超全自动系统。AI只是辅助，人类操控的无人机远胜AI系统。所以在军事领域，我们不会很快拥有大量自主机器人。反过来说，这就是为什么我认为AGI实现的最快节点是2030年。在我看来，当大规模机器人集群被应用于军事时，世界才会真正发生改变。<br>适用于推理AI的最佳GPU<br>LexFridman：让我们从AGI的话题回到之前关于AI硬件的讨论。从技术角度来看，H20这款GPU有前景吗？<br>DylanPatel：我认为我们需要深入探讨这背后的技术原因和实际情况。H20在某些方面被削弱，但在另一些方面有所提升。<br>评估AI芯片，可以关注三个核心维度：浮点运算能力（FLOPS）、内存带宽（内存容量，IO内存）和互连（芯片间通信）。这三者对构建AI系统都至关重要。AI系统需要大量计算和数据传输，包括内存访问和芯片间通信。<br>H20的FLOPS性能有所削减（纸面参数），但实际应用中，性能更接近甚至可能达到H100的60%。但在互连带宽上，H20与H100相当，而在内存带宽和容量上，H20甚至优于H100。<br>NVIDIA为何推出H20这样的芯片？我认为原因在于，H20在某些任务上表现更好，尤其是推理。推理与预训练截然不同。预训练几乎完全依赖FLOPS。当然，可以通过混合专家模型等技术来调整，例如牺牲一些FLOPS，更多地依赖互连和内存。<br>但归根结底，FLOPS仍然至关重要。衡量模型性能时，FLOPS是关键指标。例如，GPT-4的训练算力约为2e25FLOPS(2乘以10的25次方)。历史上，FLOPS一直是政府和行业关注的指标，但内存带宽和互连同样重要。尤其是在我们进入推理这个新范式后，业界在过去半年才开始认识到这一点。<br>LexFridman：推理更侧重于哪个维度？是内存吗？FLOPS的重要性降低，那互连技术呢？<br>NathanLambert：是内存。我们可以展开聊聊这里的技术细节。<br>DylanPatel：Nathan，在此之前你可以先科普一下KV缓存——我认为先解释这个概念会更好，它非常关键，KV缓存改变了模型的工作方式。<br>NathanLambert：我们需要先介绍一些Transformer的技术细节，以便大家更好地理解。<br>DylanPatel：好。首先要说明，为何内存如此重要？之前我们讨论了参数量和混合专家模型，可以通过调整有效参数与总参数的比例来嵌入更多数据，同时减少FLOPS需求。更重要的是，Transformer和注意力机制是近年来另一项重大革命。注意力机制是指模型理解上下文中所有词语之间的关系，这与参数本身无关，但必须计算。模型需要计算每个token（上下文中的每个词）与其他token的关联性。Nathan，接下来请你解释什么是KV缓存。<br>LexFridman：KV缓存是一种优化手段？<br>NathanLambert：注意力运算的核心是查询(queries)、键(keys)和值(values)，即QKV。公式中，你会看到这些矩阵的乘法运算。查询、键和值这些术语来自信息检索领域。查询是你试图检索的内容，你需要访问键来定位值，而值是被重新加权的信息。<br>实际操作中，矩阵乘法的大小与上下文长度（输入模型的token数量）相同。KV缓存本质上是模型对所有先前token的压缩表示。我们讨论的是自回归模型，模型逐个token预测。从提示词开始，例如提问“1825年的总统是谁？”然后模型就生成第一个token。<br>对于每个token，模型都执行相同的注意力操作，即乘以Q、K、V矩阵。巧妙之处在于，在重复此操作时，KV缓存允许你持续追踪之前推理出的token，并保存在内存中。在大规模推理服务中，KV缓存的管理至关重要。这方面有很多专家，可以深入探讨更多细节。<br>Transformer和注意力机制的一个关键“缺点”是内存消耗与上下文长度呈平方关系。更长的问题需要更多的内存。因此，出现了亚二次方或线性注意力的语言模型架构，例如状态空间模型(StateSpaceModels)。此处暂不深入讨论。还有一些注意力机制的创新，旨在提高内存效率和长上下文处理的精度。<br>LexFridman：这些创新有助于缓解内存限制？内存限制是主要瓶颈吗？<br>NathanLambert：是的，这些创新有助于缓解内存限制并提升性能。Gemini是目前上下文长度最长的模型，最初为100万token，现在已达200万token。你可以输入整本书，Gemini有时能从中提取事实，虽然并非完美，但正在进步。<br>实现超长上下文需要两方面努力。首先，在内存层面，需要强大的硬件架构来支持。其次，要充分发挥长上下文的性能，还需要在数据处理和注意力计算等方面进行优化。长上下文处理的主要瓶颈是内存，尤其是在需要处理大量预测时。我不确定为何输出token比输入token更昂贵，但我认为输出token需要更多计算，因为需要从模型中采样。<br>DylanPatel：我可以解释一下token定价。API通常对每百万token收取费用，输入和输出token定价不同。原因在于，输入查询时（例如一本书），模型必须先计算整本书的KV缓存。<br>这个过程可以并行处理所有token，因此速度很快。生成一个token和输入一个token的FLOPS需求相同，都需要通过模型。但输入token(预填充&#x2F;提示词)可以批量并行处理，因此更经济高效。<br>LexFridman：我看到的定价模型，输入token价格约为输出token的四分之一。<br>DylanPatel：没错。输出token昂贵的原因在于其自回归特性，无法并行处理。每次生成token，模型不仅要从内存中读取并激活，计算生成下一个token，还必须读取整个KV缓存。生成token后，需将新token及其KV缓存追加到缓存中，然后重复此过程。<br>这是一个串行操作。相比之下，预填充或提示词可以并行计算。这就是API提供商提供提示词缓存、预填充等功能的原因，因为可以降低成本并加快API响应速度。如果企业需要向API传递相同的初始内容，可以将其加载到API中并长期保存，以提升效率。<br>但这与我们讨论的推理模型有很大不同。推理模型的输出上下文长度更长。随着输出长度增加，内存使用量呈平方级增长。GPU内存最终会耗尽，尤其是在需要同时处理多个用户请求时。为了实现批量处理，即使不同用户的提示词不完全相同，系统也需要进行复杂的内存管理。<br>随着上下文长度增加，临界BatchSize（能服务更多用户的能力）会急剧下降。长上下文导致内存使用量激增，在用户数量不变的情况下，服务成本将成倍增加。<br>看下面这张图表，X轴是序列长度。即生成&#x2F;提示的token数量。例如，输入一本书可能是一百万token，而输入“天空是蓝色的”只有六个token。<br>LexFridman：我们所说的推理和思维链，正在扩展这个序列长度，尤其是输出长度。<br>NathanLambert：主要是输出长度。<br>DylanPatel：是的。三个月前，o1模型发布时，长上下文用例主要是“输入大量文档，然后获取答案”，这属于预填充场景，计算量大但并行度高，输出量少。<br>推理和AI智能体是不同的概念。用户可能只输入“执行此任务”，或者输入大量文档，但模型不仅产生少量输出，而是生成大量信息，例如思维链会持续生成大量token。因此，序列长度实际上等于模型生成的token数量，加上提示词的token数量。<br>图表显示，随着序列长度从1K增长到4K，再到16K，KV缓存的内存需求急剧增长，最终导致系统无法支持更高的序列长度，或者能服务的用户数量受限。<br>NathanLambert：BatchSize也很重要。更高的BatchSize可以提升吞吐量，实现并行处理。我们以Llama3405B模型为例，BatchSize为64，即一次服务64个不同用户。<br>DylanPatel：这样可以降低服务成本，因为服务器成本是固定的。例如，8个GPU的服务器，每小时成本是固定的。降低成本的方法有很多，但这只是一个基准。服务成本取决于你能服务多少用户，生成多少token，以及二者的比率。<br>对于推理模型，复杂性在于此，内存的重要性也由此体现。内存受限会限制用户数量和服务速度，导致服务成本急剧上升。例如，在服务器上运行Llama405B或DeepSeek-V3，用于日常聊天应用，序列长度通常只有几千token。虽然有时会处理大型文档，但处理完后就可以丢弃上下文，继续处理下一个任务。<br>然而，对于推理任务，模型需要生成数万token的序列。KV缓存必须常驻内存，持续加载和保存。这会挤占其他用户的资源。如果系统正在处理一个推理任务，内存压力会显著降低并发用户数量，导致服务成本倍增。<br>为什么DeepSeek如此便宜？<br>NathanLambert：我们再聊聊DeepSeek。DeepSeek-R1发布后，市场反响两极，服务难度可见一斑。一方面，DeepSeek自己推出了聊天应用，迅速登顶应用商店。（应用商店排名是按增速算的，不代表DeepSeek用户数超过ChatGPT，但这仍然很了不起）。即使在旧金山，大家都在热议Claude，但Claude也从未登顶应用商店，DeepSeek却做到了。他们最近还推出了API产品，提供R1模型的超长回复。<br>另一方面，DeepSeek-R1的模型权重和MIT商业友好许可证是公开的，这吸引了许多中大型公司争相为用户提供R1服务。我们也在评估R1，并尝试与我们自研的模型对比。但令人惊讶的是，所有声称提供R1服务的公司，定价都远高于DeepSeekAPI，而且大多服务不稳定，吞吐量很低。<br>DylanPatel：中国AI展现了如此惊人的能力水平，这已经让我们震惊。更令人震惊的是，他们的服务还如此便宜。之前我们已经讨论过DeepSeek训练成本低的原因——<br>LexFridman：你说到点上了。那DeepSeek为什么这么便宜？<br>DylanPatel：我认为有几个因素。首先是模型架构创新。他们采用的MLA新型注意力机制，不同于传统Transformer的注意力机制。当然，也有其他创新，例如MQA、GQA、局部注意力、全局注意力等等，都在试图改进注意力机制，降低计算复杂度。虽然复杂度仍然是二次方的，但常数项已经降低。<br>NathanLambert：与我们之前的讨论相关，MLA大约能节省80%到90%的注意力机制内存，这在长上下文中尤其重要。<br>DylanPatel：即使与目前常用的优化版本相比，MLA仍然是一项创新。<br>NathanLambert：需要明确的是，节省80%-90%内存并不意味着整个模型成本降低这么多，只是注意力机制部分的成本降低。<br>DylanPatel：没错。虽然全局注意力、滑动窗口注意力、GQMQ等技术已被广泛应用，但DeepSeek的注意力机制确实是一项架构创新，他们做了大量实验，显著降低了内存压力。虽然仍然是二次方复杂度，但内存占用已大大降低。<br>LexFridman：需要强调的是，DeepSeekR1还比OpenAIo1便宜27倍。<br>NathanLambert：我们认为OpenAI的定价包含了很高的利润。这包含多个因素，应该逐一分析。<br>LexFridman：R1输出每百万token2美元，而o1是60美元。<br>DylanPatel：DeepSeek与OpenAI定价差距悬殊。更令人惊讶的是，DeepSeek将模型开放给第三方，但第三方提供的服务价格仍然远高于DeepSeek自身。这说明成本差异不仅来自定价策略，也来自技术本身。<br>LexFridman：下面是一张图表，对比了不同平台DeepSeek-V3和DeepSeek-R1的服务价格。是什么造成了两款模型的这种差异？<br>DylanPatel：一部分原因是他们的竞争对手OpenAI利润率惊人。他们的推理服务毛利率高达75%，因为OpenAI独掌领先技术，利润空间巨大——但要知道，即使利润如此之高，OpenAI公司整体仍然亏损，仅凭这一点就造成了四到五倍的成本差距。所以维持AI服务的高成本是行业普遍现象。<br>LexFridman：他们需要这笔利润吗？用于研发投入？<br>DylanPatel：正是如此。OpenAI公司整体是亏损的，训练投入巨大。即使推理服务利润丰厚，但尚不足以弥补所有开销。所以，他们需要高利润来支撑持续研发，以及未来的持续融资。<br>LexFridman：相比之下，那DeepSeek岂不是也在巨额亏损？<br>DylanPatel：从OpenAI的情况反观DeepSeek，情况确实也是这样。<br>DeepSeek实际缺乏大规模服务能力。他们已经停止了新用户注册，服务已近乎不可用，因为GPU资源严重不足。与OpenAI动辄数十万GPU的规模相比，DeepSeek算力规模小得多，即使按我们估算的5万GPU计，还要分出一部分用于研究和量化基金业务，远不足以支撑大规模模型服务。<br>所以，R1更便宜，部分原因是OpenAI利润率高，而DeepSeek或许并未从API服务中盈利。具体情况未知，我倾向于认为DeepSeek尚未盈利。<br>这张图表也印证了这一点。看看图上的其他服务提供商，如TogetherAI和Fireworks.ai，都是顶尖公司，汇聚了Meta前员工，如FlashAttention发明者，技术实力雄厚，效率极高。据我所知，这些公司在推理服务上是盈利的，虽然利润不高。但他们的服务成本仍然比DeepSeek高出5-7倍。<br>综合来看，OpenAI的高利润率解释了约5倍的成本差异，其他商业公司的盈利需求又带来约5倍的差异。但仍然存在无法解释的成本差距。这部分差距很可能源于DeepSeek的卓越技术实力：更高效的模型架构(MLA)、MoE实现，以及其他底层优化。<br>NathanLambert：DeepSeek在训练中使用的许多底层优化库，可能也应用于推理，但这些技术并未公开。<br>DylanPatel：我补充一句——DeepSeek确实可能通过其量化基金业务来补贴模型研发，从而实现亏本运营。<br>LexFridman：所以量化基金可能在补贴DeepSeek？<br>DylanPatel：是的，极有可能。DeepSeek过去没有进行大规模融资，一直由量化基金资助。CEO持有公司50%-60%的股份。<br>NathanLambert：一些采访中提到，这种模式也是一种招聘手段。美国公司也有类似做法，拥有先进的GPU资源本身就是一种人才吸引力。站在AI前沿，对人才有强大的吸引力。<br>DylanPatel：开源也是一种招聘手段。<br>NathanLambert：Meta虽然在某些方面落后，但开源策略为他们吸引了大量人才。<br>LexFridman：聊聊网上的阴谋论，DeepSeek是否有可能精心策划了这次发布和定价，做空NVIDIA和美国AI公司股票，并巧妙地选择在星际之门发布时机发布R1，完美地利用时机获利？<br>NathanLambert：如果真是这样，那他们就太厉害了，可以掐着点算日子。但我认为这不太可能，因为从他们对AGI的愿景来看——<br>DylanPatel：他们早在12月26日就发布了V3模型。谁会在圣诞节后一天发布产品？几乎没人关注。他们在此之前就发布了V3和R1的论文，技术界一直在关注他们，然后他们发布了R1模型。<br>所以我认为他们只是想尽快发布成果，根本不在乎圣诞节或节假日。他们倒是有可能想赶在中国春节前发布，但我不认为他们是在算计市场，或者刻意制造轰动效应，他们只是在按计划发布产品。<br>NathanLambert：我认为快速发布是DeepSeek的一大优势。美国公司通常非常重视安全性，例如Anthropic，安全是他们的核心文化。Anthropic听起来是个很棒的工作场所，但如果安全至上，产品发布周期就会很长。这也是Anthropic不开源的原因之一，这是他们的官方说法。<br>内部安全审查、与英国AI安全研究所进行预发布测试等等，都会增加产品发布的流程和时间成本。但AI技术发展日新月异，速度至关重要。DeepSeek缩短了模型训练完成到发布之间的时间，评估通过后就立即发布，力求尽快展现最佳性能。DeepSeek在这方面做得非常出色。<br>DylanPatel：DarioAmodei明确表示，Claude3.5Sonnet模型大约是九、十个月前训练的。我认为他们实际上在内部测了几个月才正式发布。这中间存在巨大的时间差。尤其对于推理模型，旧金山有传言称Anthropic拥有比Claude3更强大的模型，但他们迟迟不愿发布。为什么？因为他们认为思维链技术存在潜在风险。R1模型也并非完美，有时会在中英文之间切换，甚至出现乱码，但最终能给出正确答案。对我们技术人员来说，这反而很酷，会觉得“太棒了，瑕不掩瑜！”<br>NathanLambert：这就是R1让人着迷的地方。你会觉得“这东西真厉害，不仅能跑起来，还能做到这些？”简直不可思议。<br>强化学习（RL）的魔力<br>LexFridman：我想我们可以借此机会聊聊AndrejKarpathy那篇精彩且富有洞见的推文。Andrej的思考总是充满洞见，每当他的帖子开头就要来一句“不知道这是否显而易见”时，你就知道他肯定要分享深刻的见解了。<br>原推链接：https :&#x2F;&#x2F;x.com&#x2F;karpathy&#x2F;status&#x2F;1883941452738355376<br>他在推文里写道：<br>深度学习有两种主要方式。第一种是模仿学习，观察并重复，例如预训练、监督式微调；第二种是试错学习，即强化学习，我最喜欢的例子是AlphaGo。<br>第一种方式是通过模仿专业棋手来学习。第二种是通过强化学习来赢得比赛。几乎每一个令人惊叹的深度学习成果，和所有魔力的来源，都是第二种。<br>第二种方式强大得多，也更令人惊讶。就像球拍在《打砖块》游戏中学会将球反弹到砖块后面；就像AlphaGo击败李世石；也像DeepSeek或o1等模型发现重新评估假设、回溯、尝试其他方法等策略有效时的“ahamoments”（顿悟时刻）。<br>这些策略就体现在模型的思维链中，也就是模型来回思考的方式。这些想法是涌现出来的。<br>这确实令人难以置信、印象深刻且新颖，而且有公开资料和文献记录可查。<br>模型永远无法通过模仿来学习这些策略，因为模型的认知和人类标注员的认知不同。人类永远无法知道如何正确地注释这些类型的解决策略，以及它们应该是什么样子。它们必须在强化学习过程中被发现，因为它们在经验上和统计上对最终结果有效。<br>LexFridman：文中提到了AlphaZero的类比。Nathan，你能谈谈这一点吗？以及他提到的思维链的魔力。<br>NathanLambert：回顾AlphaGo和AlphaZero的发展历程很有意义，因为它们很好地体现了模仿学习和强化学习的区别。<br>AlphaGo最初是通过学习人类棋谱来训练的，它是DeepMind系列模型中第一个达到专家级水平的围棋或国际象棋模型，其中包含一些人类数据。<br>而AlphaZero之所以被称为“Zero”，是因为它在训练过程中完全没有使用人类数据，并且AlphaZero比AlphaGo强大得多。这表明，去除人类先验知识和归纳偏置，可以使最终系统更强大。这也印证了我们之前提到的“苦涩的教训”等理念。<br>语言模型领域也有很多类似的讨论，这并不新鲜。甚至可以追溯到之前的Q-Star传闻。如果把这些信息片段拼凑起来，Q-Star可能是OpenAI开始探索类似技术的开端。<br>去年11月，关于Q-Star的传言引发了广泛的关注和好奇，人们想知道什么时候语言模型会发生类似AlphaZero的飞跃？因为我们知道这些模型非常强大，而且AlphaZero的成功也证明了这种方法的有效性。这是一个合理的类比，这种针对推理模型的新型强化学习训练方法，可能就是开启这扇门的钥匙。<br>我们还没有遇到像“第37手”（Move37）那样具有标志性的事件——所谓“第37手”是DeepMind的AI在围棋比赛中走出的一步妙招，让顶尖棋手李世石都感到震惊。虽然我们还没有遇到类似“第37手”的时刻，但这并不意味着技术路径不同，这种通用训练方法的影响仍然是极其新颖的。<br>LexFridman：你认为对于推理的思维链来说，“第37手”会是什么？<br>NathanLambert：可能是某种科学发现，或者在某个我们完全没有预料到的领域，模型利用推理能力解决了问题。<br>DylanPatel：我认为实际上可能更简单，可能与计算机应用或机器人技术有关，而非科学发现。因为关键在于模型需要大量数据来学习，它们的样本效率并不高。模型需要学习数万亿个token，这比人类一生能阅读的信息量还要多得多。人类的样本效率要高得多。婴儿通过自我对弈来学习，例如把脚放进嘴里，感受“这是我的身体”。它把手放进嘴里，用舌头上最敏感的触觉来感知手指，这就是婴儿的学习方式，不断地进行自我对弈。<br>现在我们有了类似的方法，利用可验证的反馈机制，例如代码单元测试或数学可验证任务，生成许多推理轨迹，不断分支，最后检查哪条轨迹得出了正确答案。虽然大多数轨迹都失败了，但那些成功的轨迹会被保留下来。或许还可以使用某种奖励模型来选择最佳结果。如此一来，模型在这些基准测试上的表现就会不断提升。过去六个月，许多不同的基准测试都取得了飞速的进步。<br>NathanLambert：除了前沿数学，几乎所有的数学和代码基准测试都已经被模型解决。前沿数学通常是一些对大多数人来说不切实际的难题，例如竞赛级别的开放性数学问题。但在某种程度上更贴近实际应用的数学问题，例如复杂的应用题或编程问题，正如Dylan所说，已经被模型很好地解决了。<br>DylanPatel：所以问题在于，这些方法只适用于可验证的任务。之前有一个有趣的例子，展示了将思维链应用于不可验证任务的结果，例如让人类进行闲聊，产生一个独特的想法。但这种训练方式只有在任务可验证的情况下才有效。因此，未来的发展方向可能是通过增加可验证任务的数量来扩展当前的训练方法。在数学和编程方面，编程可能还有很多发展空间，而数学方面，可验证的任务相对有限。能否创建一个求解器，然后生成推理轨迹，筛选掉无效的轨迹，保留有效的轨迹？这类问题很快就会被解决。但是，即使解决了数学问题，也并不意味着创造了真正的智能。<br>因此，我认为计算机应用或机器人技术领域会出现“顿悟时刻”，因为这些领域提供了无限的可验证的“沙盒”或“游乐场”。在网络上进行各种操作，可以产生大量可验证的反馈。模型可以从登录网站、创建账户、点击按钮等简单操作开始学习，逐渐发展到完成更复杂的任务，例如在任务平台上完成任务，或者获得大量社交媒体关注。模型可能会创建数百个账户，虽然大多数尝试都会失败，但只要有一个账户获得了大量关注，就意味着模型找到了某种规律。机器人技术也是如此，提供了无限的任务空间，从简单的“把球放进桶里”到复杂的“制造一辆汽车”。<br>我认为，最初模型的训练将在“沙盒”中进行。但最终，这种强化学习的重要性将超过大语言模型的预训练。我们可能会先预训练一个具备视觉、听觉、读写能力的多模态模型，然后让它在一个“沙盒”中无限地探索，学习数学、编程、网络导航、操控机械臂等技能。而“顿悟时刻”可能会出现在模型利用这些能力做出一些“不好”的事情时。例如，模型学会了如何使用网络，突然之间，它就能够获得大量的社交媒体粉丝和互动，因为这成为了它眼中可验证的任务之一。<br>LexFridman：也许不仅仅是获得关注，还能赚钱。模型可能实现几乎完全自动化的赚钱，例如通过成为网红、销售产品、甚至创造产品来赚取大量财富——我指的不是那种虚假的营销产品，而是真正的、有价值的产品。模型甚至可能创建并运营一家企业，并成为企业的代言人。或者创作一首热门歌曲，建立创作歌曲所需的基础设施，并成为演唱这首歌的“网红”，并创作出许多类似的歌曲。这种可能性是存在的。毕竟，我们的文化在某种程度上很看重金钱。<br>DylanPatel：而且赚钱是可验证的行为，毕竟银行账户不会说谎。<br>NathanLambert：有一些证据表明，一旦建立了收集可验证领域反馈的机制，这种方法就能奏效。在R-1之前，已经有很多关于数学问题的研究，通过增加样本数量来让大语言模型处理数学问题。可以反复尝试，观察模型做对的次数。即使是非常差的模型，有时也能给出正确答案。强化学习的核心理念就是从稀疏的奖励中学习。<br>语言空间和token空间非常庞大，无论模型是在生成语言、执行任务还是控制机器人，例如，大语言模型的token生成器可能有20万种选择，每一步都可以从庞大的空间中采样。因此，只要模型能捕捉到一点点积极的信号，并沿着这个方向学习，这就是整个强化学习领域的核心，即从稀疏奖励中学习。类似的情况也出现在数学领域，非常弱的模型有时也能给出正确答案。研究表明，可以提升这些模型在数学方面的表现，可以进行这种数学领域的强化学习训练。虽然这种方法可能效率不高，但即使是一个只有10亿参数的小模型，比DeepSeek小600倍，通过少量的这种训练，也能直接提升它在小学数学方面的表现。<br>这并不是说这种技术很快就会广泛应用。建立验证领域非常困难，并且有很多细节需要处理。但是，我们已经看到了一些初步的迹象，至少可以预期存在一个领域并且这种方法有效的可能性。<br>OpenAIo3-mini对比DeepSeekR1</p>
<p>NathanLambert：关于推理模型，核心在于数学和代码方面的推理训练。基础模型通过强化学习进行大规模推理训练，DeepSeek的R1论文对此有详细说明。他们在大规模推理强化学习后，进行了以推理为主的后训练，采用拒绝采样进行指令微调，并用奖励模型进行过滤。然后他们进行了以数学为主的RLHF。<br>哲学角度来看，一个悬而未决的问题是这种迁移能力有多强？推理训练后引入领域知识，是否能使所有模型都擅长写作？这些哲学特性是否会被激发？研究中迁移程度尚不明确。推理后的额外训练，旨在使模型更易用，这正是我们目前所看到的。因此，o3-mini和o1等模型都应用了这些技术，以迎合人类偏好。<br>DylanPatel：大家可能忽略了，Google的Gemini2.0FlashThinking比DeepSeekR1更经济高效，且性能更优，并且在12月初就已发布。</p>
<p>NathanLambert：GeminiFlashThinking风格迥异，不如o1那样具表现力，或说“赛道”较窄。Qwen和DeepSeek此前也分别发布了推理预览模型QwQ和R1-Lite。这些模型更像在特定轨道上，专注于数学和代码，而o1则更灵活，能回答各种问题，尽管在某些任务上可能不完美，但其优势在于全面性。关键在于如何衡量模型的成熟度，判断其是否能应用于所有场景。GeminiFlash的训练方式与o1不同，它更像是在现有流程中融入推理。Gemini团队快速迭代，GeminiFlash推理已是节假日后的第二个版本，发展迅速。而构建大规模强化学习训练流程则需更长时间。<br>在o1完全开放之前，我们就已在AI2研究过类似问题。R1本质上是通过强化学习微调，我们在Tülu系列模型中也使用了类似方法，虽然可以引出相似行为，但由于强化学习在训练后期，推理表达相对较弱。强化学习训练的投入量决定了输出效果。<br>LexFridman：我们现在就用Gemini2.0FlashThinkingExperimental121提问来试试。<br>DylanPatel：本文第一段我们不是问了问DeepSeek对人类的看法吗？可以把同样的问题再抛给Gemini。<br>NathanLambert：生成结果出来了。它的总结是，人类是“自我驯化的猿类”。<br>LexFridman：它也会回顾推理过程吗？这视角很新颖。<br>DylanPatel：点击展开看看。<br>LexFridman：分析请求……Gemini的关键词是“新颖”（Novel）。<br>NathanLambert：看起来和DeepSeek的思考过程有些不同，像一个更规范的输出。<br>LexFridman：结构更清晰，更有条理。<br>DylanPatel：它先是专注思考“人类”这个关键词，然后是“有机体”……<br>LexFridman：“顶级掠食者，关注驯化，将驯化应用于人类，探索自我驯化概念”。<br>NathanLambert：有意思。<br>LexFridman：“提炼，阐明见解：更强的面部表情和沟通能力，可塑性和适应性，对社会群体的依赖，还有自我批评，进一步提炼。结论是，人类不仅是社会性动物，更是深度自我驯化的猿类。这种自我驯化是理解我们独特认知和社会能力的关键。”《自我驯化的猿类》可以当一本书的名字了，我很喜欢。<br>NathanLambert：我其实更倾向于DeepSeek给出的答案。<br>DylanPatel：回到Nathan的观点，推理模型确实存在粗糙感，即使R1相比o1也是如此。早期的FlashThinking版本也有类似问题，在很多方面不够完善。虽然通过验证器和强化学习提升了数学和编码能力，但在某些方面有所损失。o1在很多方面也不如Chat模型。<br>NathanLambert：但差距不大。<br>DylanPatel：R1在某些方面不如V3，强化学习提升了表达能力，但也可能在其他方面有所削弱。这是模型间以及o1功能上的重要区别。OpenAI的o1-pro和o3在思维链之上叠加了搜索功能。解决ARC-AGI挑战不仅靠思维链，还包括多次采样和选择。<br>NathanLambert：并行运行是搜索吗？我们对o1-pro的工作原理信息有限，不足以断定是搜索。<br>DylanPatel：应该是并行采样，然后选择结果。<br>NathanLambert：我们不清楚他们采取的选择机制。自o1发布以来，蒙特卡洛树搜索技术备受关注，它将思维链分解为中间步骤。思维链的概念源于早期的论文，即通过“逐步验证”来引导模型生成步骤列表。现在，思维链已成模型标配，无需特别指示。蒙特卡洛树搜索会在思维链中选取中间点进行扩展，消耗更多计算资源，最终选择最佳结果。这是一种复杂的搜索形式，曾在MuZero和AlphaZero等系统中使用。<br>DylanPatel：另一种搜索方式是询问多人，取多数答案。搜索形式多样，可繁可简。我们不确定具体机制，但OpenAI显然不只是按顺序生成思维链，而是在并行启动多个。在ARC-AGI测试中，他们并行启动上千个，正确率从单次启动的30%提升到80%-90%。<br>NathanLambert：推理模式还有很多扩展空间。目前，语言模型旨在单次响应中给出最高概率的正确答案。我们正在探索模型推理的新方式，需要重新评估训练过程，这会带来更多进步。但OpenAI的具体做法尚不明确，可能是更多采样和多项选择，也可能是更复杂的技术。但可以肯定的是，他们改变了训练方式，并意识到推理模式将有所不同。<br>LexFridman：这种计算密集型探索在财务上可行吗？<br>DylanPatel：关键在于，在实现AGI之前，强大的人工智能已渗透到经济的方方面面。GPT-3在2020或2021年训练，推理成本高达每百万token60-70美元，单位智能成本极高。两年后，成本降低了1200倍，达到与GPT-3相当的智能水平。看下面这张图表。<br>现在成本约为5美分，之前是60美元，成本大幅下降。DeepSeek的低价引发关注，但实际上，他们并未低于成本下降的趋势线。GPT-3是首个达到此水平的模型，意义重大。GPT-4的推理能力又将如何发展？这将是架构创新、更优质数据、更优训练技术、推理系统和硬件升级的综合结果。成本曲线将持续下降。<br>未来，我们或许可以生成上千个大语言模型，从中选择最优结果，或者采用蒙特卡洛树搜索等技术。虽然现在成本高昂，但会逐渐降低，这将释放人工智能的潜力。进步速度非常快，问题在于“何时”而非“是否”。GPT-4发布时推理成本约为每百万token60美元，现已降至2美元左右，未来可能降至几美分，并达到GPT-4的质量水平。这为o1等推理模型，以及o1-pro、o3等搜索技术奠定了基础。这些技术目前成本过高，但会变得更经济，这将解锁更高级的智能。<br>唠唠英伟达<br>LexFridman：大模型的价格会越来越便宜。DeepSeekR1的发布让大家感到震惊，因为它价格很低。一个直接的体现是NVIDIA的股票下跌。你能解释一下发生了什么吗？以及当前的情况，NVIDIA是否还能保持领先地位？<br>NathanLambert：在座各位应该都是看好NVIDIA的。我认为某种程度上，市场反应是合理的。NVIDIA在美国最大的客户是科技巨头，他们在人工智能领域投入巨资。如果用更简单的方式理解DeepSeek的影响，那就是它表明可以用更低的成本获得非常好的模型。这样来看，市场可能会认为大型科技公司在人工智能上的支出或许会减少，股价因此下跌。<br>实际情况更复杂，还包含社会因素。比如，应用商店排名的上升和社交媒体传播都在加速信息扩散。我不做交易，对金融市场也不熟悉，但这件事在周末发酵，社会压力不断积累。如果发生在工作日，可能有多天的交易缓冲，但周末效应导致大家集中抛售，形成了恐慌性蔓延。<br>DylanPatel：我认为存在很多误解。比如，有人说“这些公司在模型上花了数十亿美元”，但实际上并没有。目前还没有哪一款公共模型花费超过10亿美元。GPT-4花费了数亿美元，之后他们通过4o、4Turbo等版本降低了成本。但10亿美元级别的模型训练（包括预训练和后训练）即将到来。<br>另一个误解是，“DeepSeek的成本不包括所有方面”。他们的确没有计入许多研究成本和其他费用。很多成本实际上花在了推理和后训练上。OpenAI“数十亿美元”的投入中包含了研究、工资等所有成本，而DeepSeek声称的“600万、500万美元”的成本中并没有包含这些。<br>所以，大家对这些数字存在误读。此外，NVIDIA股价一直上涨，市场也在寻找各种理由来压低其股价。我不是说要打压NVIDIA。但大家都在寻找卖出或担忧的理由，例如Blackwell芯片的延迟交付，以及关于“扩展定律终结”的讨论。这很讽刺，因为之前一直有声音说“模型进步停滞了”，暗示没必要再投入更多资金，预训练扩展已死。<br>NathanLambert：这种论调持续了一段时间。<br>DylanPatel：对，然后R1出现了。现在风向又变了，变成了“模型进步太快了！放慢速度，别再GPU上砸钱了！”<br>但最有趣的是杰文斯悖论正在应验。自从V3发布，特别是圣诞节后，AWS的H100定价持续上涨。H200几乎完全缺货，因为它内存更大，R1更倾向于使用H200而不是H100。<br>LexFridman：为不熟悉的朋友解释一下，杰文斯悖论指的是，当效率提高时，总资源消耗反而会增加，这有点反直觉。<br>NathanLambert：这周我们临时想买一些GPU做演示，结果发现并不容易。我们本来只是想买16或32个H100，但现在很难搞到。<br>DylanPatel：半导体行业经历了50年的摩尔定律，每两年成本减半，晶体管数量翻倍，像时钟一样精准。虽然速度放缓了，但半导体行业一直在增长，只是有周期性波动。我认为人工智能领域也会如此，有起伏是正常的。但在人工智能领域，发展速度太快了。过去是每两年翻一番，现在是三年内提升1200倍。这种进步幅度难以想象。<br>LexFridman：我很困惑，因为在我看来，NVIDIA的股票应该上涨。但如果只看实际的表现，杰文斯悖论就很明显地在发挥作用。<br>AI基础设施的“隆中对”（纵观全美的AI超级集群）<br>LexFridman：Dylan，你收集了很多关于主要AI公司大型集群的信息。能谈谈这些集群的构建情况吗？<br>DylanPatel：这些大型集群的规模前所未有，这才是重点。美国数据中心的电力消耗一直在缓慢上升，云计算时代也只上升到2-3%，几十年都保持在这个比例。但现在情况变了，到本十年末，即使保守估计，到2028-2030年，数据中心电力消耗可能占到美国总量的10%。<br>传统数据中心领域的人可能觉得这不可思议，但像Anthropic和OpenAI这样真正关注AI的公司会说：“还不够。”<br>这包括分散在美国各地的和集中式的集群，其中大部分是分布式部署。比如，Meta或OpenAI增加了一个千兆瓦的算力，但主要是通过在美国各地分布数据中心实现的，用于推理和其他应用。<br>LexFridman：也许我们应该先解释一下什么是集群。它是否包括AWS？不同类型的集群有哪些？你所说的大型集群具体指什么？什么是GPU？什么是计算？<br>DylanPatel：传统上，数据中心和数据中心任务是分布式系统问题，可以非常分散。比如，你向Google发送一个请求，它会被路由到离你较近的数据中心，执行搜索排名等任务，然后返回结果。但现在任务性质正在快速改变，人们真正关注的是推理和训练，而不是数据库访问或广告投放。<br>推理和传统分布式系统非常不同，但又更简单。推理仍然像是，“我要在这些数据中心的各个区域部署数千个GPU”。用户提交请求，服务启动，比如在Word中使用Copilot，或者WindowsCopilot，或AppleIntelligence。请求被发送到数据中心，数据中心处理后返回结果。这将是计算的主要部分。<br>我们一直在追踪数千个正在构建的数据中心，它们是主要增长点。但真正改变游戏规则的是规模，正是规模驱动了对数百万GPU的需求。当我们回顾AI发展史，在2个或4个GPU上运行AlexNet就已经是大事了。<br>NathanLambert：因为当时破天荒用了GPU，而且是多GPU。<br>DylanPatel：自那之后规模一直在扩大，先是GPT-3，然后GPT-4使用了20,000个A100GPU。这在规模和成本上都是前所未有的，GPT-4的训练运行花费了数亿美元，带来了惊人的性能提升，完全符合扩展定律的预测。<br>但20,000个A100GPU这个数字本身并不夸张。每个GPU约400瓦，加上服务器和其他组件，总功耗约15-20兆瓦，这只是标准数据中心的大小。前所未有的是，所有GPU都集中运行一项任务。<br>NathanLambert：可以通过类比来给读者科普一下，运行一台烤面包机是多少瓦？<br>DylanPatel：烤面包机的功耗和A100差不多。后来有了H100，单GPU功耗从400瓦提升到700瓦，加上其他组件，总功耗约1200-1400瓦。<br>LexFridman：所以除了GPU，还需要大量电力。又因为会产生大量热，需要冷却。此外，为了连接GPU，所以还需要强大的网络。<br>DylanPatel：我跳过了介绍这些细节，因为数据中心本身就很复杂。但对于GPT-4的规模来说，仍然可以使用标准大小的数据中心。现在，看看去年人们构建的集群规模，范围很广。一种方案是“使用标准数据中心，用大量光纤连接多个数据中心”，OpenAI和微软在亚利桑那州就是这么做的，他们有10万个GPU。<br>Meta也类似，采用标准数据中心设计，将多个数据中心连接在一起。他们最初用了24,000个GPU，但只有16,000个用于训练，因为GPU可靠性不高，需要备用GPU替换。现在，他们正用大约128,000个GPU训练Llama4。<br>想想看，10万个GPU，每个1400瓦，就是140兆瓦；12.8万个就是150兆瓦。从2022年到2024年，两年时间，功率需求从15-20兆瓦跃升到150兆瓦，几乎增长了10倍。<br>再看马斯克，他承认自己在预训练大语言模型方面入场较晚，xAI成立较晚。但他全力以赴建设数据中心，打造了世界上最大的集群，20万个GPU。他在孟菲斯买了一个工厂，升级变电站，同时使用大量移动发电机和单循环燃气轮机，接入工厂旁的天然气管道，大量燃烧天然气来发电。马斯克在一个废弃的电器厂里部署了20万个GPU。<br>所有超大规模企业都在朝着更大规模迈进。马斯克还在工厂旁边自建天然气发电厂，部署特斯拉Megapack电池组来平稳供电，并使用工业冷水机为芯片水冷。所有这些疯狂的举措都是为了让集群变得更大。<br>再看OpenAI在德克萨斯州阿比林的星际之门项目，至少他们是这么宣布的。虽然还没建成，马斯克说他们缺钱，对此有些争议。但就规划规模而言，至少第一部分肯定有资金支持，而且还有后续扩建计划。整个数据中心规划电力输入2.2千兆瓦，其中约1.8千兆瓦用于芯片供电。<br>这是一个惊人的规模，2.2千兆瓦比大多数城市都多，这只是一个用于训练的集群的电力需求，为了预训练、后训练等等。<br>LexFridman：太疯狂了。<br>DylanPatel：重点是——大家都在这么做。Meta在路易斯安那州也建了两个大型天然气发电厂，以及大型数据中心。亚马逊和Google也有类似规模的计划，xAI也有。所有这些竞争者都在努力构建多千兆瓦的数据中心。因为他们相信，“如果我现在拥有更多算力……”<br>所以，预训练扩展还会继续，但会放缓。而后训练，比如模型与环境互动、自我对弈等，将成为算力消耗的主要方向。他们不断探索各种方法，让AI更强大，因为算力投入和模型性能之间仍然存在正相关性。<br>虽然扩展定律存在收益递减，算力增加10倍，模型性能不会提升10倍，但效率也在提升，可以延缓收益递减的到来。这些大规模数据中心对电网造成巨大压力。<br>NathanLambert：核电站在里面扮演什么角色？<br>DylanPatel：Nathan应该是想说那个新闻，亚马逊试图收购核电站Talen。Talen的股票一直在飙升。他们也在那里建设大型多千兆瓦数据中心。<br>有趣的是，美国某些地区的输电成本甚至高于发电成本，因为电网建设速度太慢了。对电力的需求激增，但电力传输能力却跟不上。在弗吉尼亚州等地，输电成本高于发电成本，这引发了很多意想不到的次生影响。<br>LexFridman：电网能支撑这种增长吗？<br>DylanPatel：近两届政府都有行政命令，希望能减少监管，加速基础设施建设。但这仍然是一个巨大的挑战——我们能否足够快地建设足够的电力供应？<br>LexFridman：基本上需要在每个数据中心旁边都建一个核电站？<br>DylanPatel：有趣的是，建造发电厂太慢了，无论是新建还是改造现有电厂。所以，必须另辟蹊径。数据中心的电力成本占比其实不高，集群成本中，电力通常只占不到20%，大部分是资本支出和GPU折旧。<br>所以，大家会想，“管他的，先建天然气电厂再说。”Meta在路易斯安那州，OpenAI在德克萨斯州，还有的公司是与能源公司合作。<br>当然，也有更环保的方案。马斯克在孟菲斯的做法是极端例子，他不仅使用高效的双循环燃气轮机，还用效率较低的单循环燃气轮机和移动发电机。但另一方面，太阳能和风能发电的相关性不同，如果将它们与电池储能和天然气发电结合起来，有可能更环保地运行数据中心。只是转型速度较慢。<br>Meta基本上选择了“无所谓可持续发展承诺”的策略，或者他们会购买电力购买协议（PPA），声称使用了风能或太阳能，但实际上只是购买电网的电力。微软也放弃了一些可持续发展承诺。也许AGI能解决全球变暖之类的问题，谁知道呢——所以，这就是实验室里的人们的态度，“用天然气就用天然气吧，这场竞赛太重要了，输不起。”<br>LexFridman：我正好前段时间（采访马斯克的时候）有机会参观了孟菲斯数据中心。那里的团队和创新速度令人难以置信。我感觉从来没有人做过这种规模的事情，更没人以xAI的速度做到过。他们在不断解决瓶颈，优化各个环节。数据中心集成了系统管理、机器学习等各种复杂技术。真正支撑这一切的是那些精通底层软硬件、网络的人。他们必须确保一切运行正常，并制定完善的测试流程。我记得他们好像在用以太网做网络连接。<br>DylanPatel：他们用的是NVIDIASpectrum-X以太网。我认为真正的幕后英雄是冷却和电气系统，这些往往被忽视。<br>我想讲个故事，或许能说明这些系统的疯狂程度。训练模型时，最简单的理解就是多次迭代，然后交换所有信息并同步权重，这就是训练中的一个步骤。每一步，损失函数都会下降，但并非总是如此。<br>简单来说，模型会进行大量计算，然后进行权重交换。GPU功耗占比很大，网络功耗相对较小。计算时，GPU功耗很高，但交换权重时，如果通信和计算不能完美重叠，GPU可能会空闲，这时模型在更新权重。所以，功率消耗会出现峰值。<br>有趣的是，数据中心功率规模如此之大，很容易发生意外。Meta曾经不小心向上游提交了一段PyTorch代码，添加了一个操作符——我没开玩笑，这段代码写着“PyTorch.powerplantnoblowupequals0orequal1”。作用是在交换权重时，让GPU计算一些虚假数字，避免功率激增，导致电厂过载，因为瞬时峰值会搞砸一切。<br>LexFridman：有道理。必须采取这种措施，确保系统稳定运行。<br>DylanPatel：马斯克的解决方案是，“多用特斯拉Megapack电池组！”<br>LexFridman：是的，象征意义大于实际作用。<br>DylanPatel：大家有不同的解决方案，Meta的方案是公开的，通过代码层面控制功率峰值。这个操作符的作用就是让GPU在特定时刻空转，避免功率飙升。<br>LexFridman：但这也能让你意识到自己正在使用多少电力。太疯狂了。能再谈谈冷却吗？我知道马斯克坚信在所有情况下都应该使用液冷，这算是一种新趋势。很多人不用液冷。冷却方面有什么值得关注的？<br>DylanPatel：风冷一直是主流标准，用金属热管和风扇散热就足够了。但人们一直在尝试液冷。Google的TPU就是液冷的，他们已经用了好几年。但GPU大规模液冷，尤其是xAI这种规模，前所未有。下一代NVIDIA顶级GPU必须采用液冷，这是必然趋势。<br>马斯克在这一代GPU上就实现了大规模液冷，需要大量配套设施。看看孟菲斯工厂的卫星照片，你会看到很多外部水冷机组，看起来像半挂车集装箱，实际上是水冷器，有大约90个这样的水冷集装箱放在外面，冷却水，再送回数据中心，分配到各个芯片，带走热量，再循环回来。液冷不仅能有效散热，还能提高效率。<br>回到内存带宽、FLOPS和互连这三个维度，芯片距离越近，越容易实现高速互连。这也是液冷的优势之一，可以将芯片更紧密地排列，实现更高速度的连接。<br>LexFridman：现在谁的集群最大？未来谁会是最大的？<br>DylanPatel：目前，单个集群最大的是马斯克的孟菲斯集群，20万个GPU。Meta有12.8万个，OpenAI现在是10万个。需要说明的是，其他公司拥有的GPU总量可能比xAI多，但他们没有把所有GPU都放在一个地方。训练模型需要GPU紧密连接。虽然有技术正在研究跨区域训练，但大多数情况下，还是希望GPU集中在一个区域，通过高速网络连接。<br>但今年，大家都在建设更大规模的集群。Anthropic和亚马逊正在合作建设一个包含40万个Trainium2芯片的集群，这是亚马逊自研芯片，旨在摆脱对NVIDIA的依赖。Meta和OpenAI的目标也是数十万规模。明年，可能会出现50万到70万GPU的集群。而且，这些GPU的功耗更高，Hopper架构700瓦，Blackwell架构达到1200瓦——单芯片功耗在增长，芯片数量也在增加。<br>LexFridman：太疯狂了。马斯克说要干到一百万GPU，你认为这真的可行吗？<br>DylanPatel：我从不怀疑这个男人。他提交的发电厂和特斯拉电池组文件表明，他对孟菲斯确实有宏伟的计划。许可证等信息是公开的，只是时间表尚不完全清楚。但我从不怀疑马斯克，他总是能带来惊喜。<br>LexFridman：这些大规模集群的用途是什么？如果未来有百万GPU级别的集群，比如2-3年内实现，有多少比例用于训练？预训练占多少？实际计算又占多少？<br>DylanPatel：这些大型集群不适合推理——当然也可以做推理，但主要目标是训练。推理算力更多是分布式的，比如“我这里有个30兆瓦的数据中心，那里有个50兆瓦，再远一点还有个100兆瓦。”推理任务分散在各地。而大型集群，多千兆瓦级别的数据中心，主要用于训练，因为所有GPU都集中在那里，可以通过超高速网络连接，满足训练的需求。<br>预训练是之前的重点，可以通过增加参数和数据来提升模型性能。但现在预训练的扩展空间有限，因为高质量数据已经不多了。当然，视频、音频和图像数据尚未充分挖掘，还有扩展潜力。但很多人已经从YouTube视频中提取了转录文本，数据量已经很大，只是视频和图像数据蕴含的价值尚未完全释放。<br>预训练还有扩展空间，但后训练才是未来算力消耗的主要方向。模型将与自身对弈，进行自我博弈，执行可验证的任务，在沙盒环境中进行计算机使用，甚至模拟机器人操作。这些都将是“后训练”的应用场景。我认为未来“后训练”会逐渐取代“预训练”成为主流——在某个时间点，预训练和后训练会融合。而过去几年，预训练规模远超后训练。但对于可验证的任务，特别是那些可以无限扩展的任务，比如计算机使用、机器人技术等，你可以在这些领域投入无限的计算资源。<br>NathanLambert：尤其是在上下文长度不断增加的情况下。预训练的终点就是增加模型上下文长度。正如我们前面讨论的，长输入比长输出更容易处理。许多后训练和推理技术都依赖于大量采样，而且上下文越来越长，计算效率实际上在下降。<br>FLOPS是衡量算力的标准，但在强化学习中，模型权重更新方式与预训练和生成式模型不同，效率会降低，FLOPS可能不再是合适的指标。当然，随着基础设施的进步，效率可能会再次提升，FLOPS可能又会变得有用。<br>LexFridman：我们一直在讨论的这些技术，基本上都围绕NVIDIAGPU，对吧？GPU领域有竞争对手吗？<br>DylanPatel：Google在TPU方面有点被忽视了。其实TPU很棒，性能很好。但不知为何，Google在数据中心建设方面显得有些犹豫。当然，他们也在建大型数据中心，实际上他们拥有最大的集群，我说的是NVIDIA集群。Google拥有最大的集群，但他们的做法很有趣。他们有两个数据中心超级区域，数据中心并不集中在一个地方，而是分散在约30英里的范围内。不是GPU，而是TPU。在爱荷华州和内布拉斯加州，他们有四个数据中心彼此相邻。<br>LexFridman：为什么Google不公开展示他们的集群规模？<br>DylanPatel：你可以搜索“多数据中心训练”（multi-datacentertraining），能搜到一些很好的照片。<br>上图就是一张标准Google数据中心的照片，有点像半分布式多数据中心架构——他们的数据中心看起来和别人的很不一样。<br>LexFridman：这图是什么意思？<br>DylanPatel：图片中间那些大的矩形框，是存放芯片的地方。往下看，你会看到水管，顶部是冷却塔，还有一堆柴油发电机，作为备用电源。数据中心本身看起来比水冷器还要小。芯片可以更紧密地排列，但冷却大量水非常困难。<br>Google的基础设施非常先进，TPU基础设施是独一无二的。他们所做的就是在几个区域建了多个这样的数据中心。比如，再往下看，就是微软在亚利桑那州的数据中心，GPT-5“可能在那里训练”。<br>这些数据中心都位于同一区域，内布拉斯加州、爱荷华州，他们在俄亥俄州也有类似的集群。这些数据中心彼此非常靠近，他们用光纤将它们以超高带宽连接起来。所以，这就像一堆数据中心。<br>关键是，Google拥有非常先进的基础设施，在一个小区域内紧密相连。Elon的集群是最大的完全集成的集群，因为所有GPU都集中在一栋建筑里，他在这方面是对的。Google拥有最大的集群，但分散在三个地点，距离较远。<br>LexFridman：为什么Google不与NVIDIA竞争GPU市场？他们为什么不卖TPU？<br>DylanPatel：我认为有几个原因。首先，TPU最初是为了降低搜索成本，优化搜索模型而设计的。Google的GPU和TPU采购和使用，很大一部分都是为了内部工作负载，比如搜索、现在的Gemini、YouTube、广告等。这些都是TPU的主要应用场景，也是他们重点优化的方向。因此，TPU架构针对Google的特定用例进行了优化，在其他方面可能不那么通用。<br>举个例子，他们开源了一个名为Gemma的模型，号称Gemma-7B，但实际上有80亿参数，因为词汇表太大。他们把词汇表做得这么大，是因为TPU的矩阵乘法单元很大，这是他们优化的重点。所以他们觉得，“词汇表也做大点吧”。尽管对于这么小的模型来说，大词汇表没什么意义，但这符合他们的硬件特性。Gemma在GPU上的运行效率不如Llama，反之亦然，Llama在TPU上的效率不如Gemma。<br>硬件和软件是协同设计的。Google所有的搜索模型、排名和推荐模型，以及其他各种AI模型，都在TPU上进行了深度优化。但这些软件栈并没有完全公开，只开源了一小部分，比如JAX和XLA。在Google内部，研究人员在TPU上训练模型，很多时候不需要了解硬件细节，体验很好。<br>NathanLambert：他们都很喜欢TPU。<br>DylanPatel：但是一旦你离开Google（就没法用TPU了）——<br>NathanLambert：所以很多人离开Google后又回去了。<br>DylanPatel：是的，许多人离开Google创业，脑子里全是惊人的研究想法。但他们很快发现，“基础设施和软件太难搞了”，尤其是在GPU上。如果他们尝试使用TPU，也会遇到同样的问题，因为他们无法访问Google内部的代码库。所以，你如何说服一家靠搜索业务“下金蛋”的公司，放弃数千亿美元的搜索利润，转而去销售GPU或TPU？他们过去每年只采购几十亿美元的硬件，现在采购额增加到100-150亿美元，但你如何说服他们再增加一倍采购量，并想办法把硬件卖出去，赚300亿美元？区区300亿美元，谁会在乎？<br>LexFridman：300亿美元的硬件销售额最终可能还不如搜索业务的利润高？<br>DylanPatel：服务业务的利润率总是更高。需要明确的是，现在硬件支出超过了服务支出，因为硬件投资是先行投入。<br>LexFridman：这是在投资未来。<br>DylanPatel：如果AI业务没有盈利，或者盈利不足，泡沫肯定会破裂。硬件厂商不可能永远烧钱。NVIDIA正试图通过销售和授权软件来提升价值链。但Google从来没有“销售硬件产品”的基因。GoogleCloud（谷歌云）是一个独立的部门，与TPU团队是分开的，TPU团队又与DeepMind团队分开，DeepMind团队又与搜索团队分开。Google内部有很多官僚机构。<br>LexFridman：GoogleCloud是一个独立于TPU团队的团队？<br>DylanPatel：从技术上讲，TPU隶属于基础设施部门，而基础设施部门又隶属于GoogleCloud。但GoogleCloud主要负责云服务租赁业务。<br>DylanPatel：GoogleCloud负责云服务租赁，TPU架构的目标非常明确，硬件和软件都是为内部需求定制的。JaxXLA团队不为外部GoogleCloud客户提供服务。而NVIDIA的CUDA团队，比如NCCL，则为外部客户提供服务。像Jax和XLA这样的内部团队，更多服务于DeepMind和搜索部门，他们的客户群体不同，他们不是在为外部客户构建产品。<br>LexFridman：你认为这就是为什么AWS一直在云服务领域领先于Azure，而GoogleCloud却落后的原因吗？GoogleCloud相对于AWS而言规模很小吗？<br>DylanPatel：GoogleCloud排名第三。微软是第二大，但亚马逊是最大的。微软“欺骗性地”将MicrosoftOffice365等产品以及企业级许可证都算作云服务收入，所以实际上差距更大。不过，微软仍然是第二，亚马逊遥遥领先。为什么？因为AWS更好用、更容易上手，而且在很多情况下更便宜。<br>NathanLambert：而且它是第一个进入市场的。<br>LexFridman：但先发优势并不总是意味着能保持领先。<br>DylanPatel：AWS贡献了亚马逊超过80%的利润，甚至可能超过90%。亚马逊的电商配送中心就像，“总有一天我们会决定从中赚钱”，但他们还没开始真正盈利。他们只赚取微薄的利润。<br>NathanLambert：我总觉得有一天AmazonPrime会涨价三倍。<br>LexFridman：可以先期待他们改进AWS的用户界面，因为它实在太糟糕了，非常笨重。<br>DylanPatel：我认为Google产品的用户界面有时还挺好用的，但他们只关心顶级大客户的反馈。<br>LexFridman：所有这些公司，他们都为大客户优化服务，目标客户是企业。<br>DylanPatel：亚马逊也一直兼顾小客户。早期，他们会参加各种湾区活动，免费发放积分，或者直接用信用卡就可以开始使用AWS。业务是与客户共同成长的。为什么Snowflake能够遍布亚马逊云平台？因为Snowflake在早期亚马逊还不太重视他们的时候，就开始使用AWS。当然，后来Snowflake和亚马逊建立了非常庞大的合作伙伴关系，但最初就是这样。亚马逊的用户体验和产品质量更好。<br>此外，他们自研的芯片降低了传统云服务（存储、CPU、网络等）的成本结构，包括数据库服务。我认为亚马逊收入排名前五的Google产品中，毛利润最高的产品，有四个都与数据库相关，比如Redshift等。亚马逊建立了一条从芯片设计到用户体验的完整流程，AWS是其中的关键环节。的芯片团队，他们在内部拥有很棒的芯片，TPU、YouTube芯片以及其他一些芯片。问题在于，他们不为外部客户提供服务，他们只为内部客户服务。<br>NathanLambert：NVIDIA的整个公司文化都是为此而生的。最近有一本书，《NVIDIA之道》，详细介绍了NVIDIA如何提前布局未来机会，并准备好CUDA软件库，让高性能计算的新应用能够快速在CUDA和NVIDIA芯片上发展。这与Google这种服务型公司完全不同。<br>LexFridman：必须承认，NVIDIA是一家非常特别的公司，他们的文化和组织结构确实是为这类事情优化的。说到这里，硬件领域有人能挑战NVIDIA吗？英特尔？AMD？<br>DylanPatel：我个人认为很难。我们花了很多时间与AMD合作，测试他们的GPU在训练和推理方面的性能。他们的硬件还不错，在很多方面甚至比NVIDIA的好。问题在于他们的软件生态系统太糟糕了。当然，他们正在努力改进，速度也越来越快，但差距太大了，而且他们在这方面投入的资源不够，至少过去是这样。也许他们现在正在改变策略，但在合作的几个月里，我们提交的bug数量最多，简直难以置信。为什么？因为他们只关心最大的客户，会给大客户发送定制的私有镜像。对于使用PyTorch并想用公开库的小客户，他们不太在意。AMD正在进步，但我认为他们无法撼动NVIDIA的地位。英特尔现在的情况更糟，他们需要想办法自救。<br>LexFridman：能详细说说英特尔“情况更糟”的原因吗？<br>DylanPatel：这就得回到之前的话题，目前只有三家公司能进行先进制程的芯片制造：台积电，三星以及英特尔。三星做得不好，英特尔也做得不好。我们可能正走向一个全世界只有一家公司能进行先进制程研发的世界，而这家公司已经占据了大部分芯片制造市场份额。<br>就英特尔而言，他们一直在缓慢而稳定地衰落。他们在服务器和PC市场仍然领先，但现在苹果推出了M1芯片，NVIDIA也在发布PC芯片，高通也在进军PC市场。<br>在服务器领域，超大规模企业都在自研基于ARM架构的服务器芯片，而英特尔在AI芯片领域也没有突出成就，只有一些小小的进展。他们错失了移动市场，拒绝了iPhone合作，所有这些因素叠加，导致他们失去了制程技术的领先地位。<br>他们曾经领先20年，现在至少落后了好几年，他们正在努力追赶，18A、14A工艺制程战略能否奏效还有待观察，他们能否超越台积电也未可知。英特尔目前亏损严重，他们刚刚解雇了CEO，尽管这位CEO是唯一真正了解这家公司的人。他或许不是最优秀的CEO，但算是一个相对懂技术的领导者。<br>LexFridman：英特尔的主要收入来源是什么？仍然是CPU吗？<br>DylanPatel：PC和数据中心CPU。但数据中心CPU市场正在转向云计算，亚马逊、微软、Google都在自研基于ARM架构的CPU。在PC市场，AMD也在抢占市场份额，NVIDIA也推出了PC芯片，虽然可能不会成功。联发科、高通也曾尝试过。苹果在PC芯片领域表现出色。英特尔在PC市场可能会受到挤压，尽管传统上WindowsPC仍然会坚持使用英特尔CPU。<br>AGI竞赛的赢家会是谁？<br>LexFridman：我们来聊聊AGI竞赛。你们认为谁会胜出？之前我们提到了Google和Meta。<br>NathanLambert：通常来说，大家认为Google领先，因为他们有基础设施优势。<br>LexFridman：但新闻里总是说OpenAI才是领头羊。<br>NathanLambert：他们在舆论宣传上更胜一筹。<br>DylanPatel：我觉得他们的模型也确实是最好的。<br>NathanLambert：没错，他们确实有用户能直接体验到的最佳模型，而且是公认的行业标杆。<br>DylanPatel：而且他们的人工智能收入也是最高的。<br>LexFridman：现在人工智能领域谁在真正赚钱？有人盈利吗？<br>DylanPatel：从账面利润看，微软是盈利的，但他们在资本支出上投入巨大，这些支出需要在多年内摊销。<br>Meta通过推荐系统赚了很多钱，这当然也属于人工智能，但不是Llama的功劳。Llama肯定还在亏损。<br>Anthropic和OpenAI显然也没盈利，不然他们就没必要继续融资了。尽管理论上他们有可能盈利。GPT-4投入了数亿美元，但也在产生数十亿美元的收入。所以盈利是可能的。但他们必须持续研究，提升计算效率，降低成本，就像GPT-3那样实现1200倍的效率提升。也许现在只提升了几百倍，但GPT-4Turbo和4o已经更便宜了，未来可能还会有更经济的版本。<br>LexFridman：而且这些研究投入巨大。我觉得在讨论模型成本时，大家常常忽略研究和人力成本，不仅仅是训练和测试的费用。<br>DylanPatel：没错。比如，为了实现现在的推理功能，他们需要不断扩展，还需要投入大量研究。大家关注回报很正常，但逻辑其实很简单：GDP等于劳动力和工业资本的结合。如果智能变得廉价，就能驱动经济大幅增长。投资的逻辑就在这里。<br>我认为目前只有NVIDIA真正赚得盆满钵满，其他硬件供应商也一样。超大规模数据中心账面上可能盈利，但实际上他们购买GPU的支出更多。而且谁也无法保证两年后，他们还能从每块GPU上赚回这么多钱。OpenAI可能会突然倒闭，微软投给OpenAI的GPU可能就没了客户。虽然这只是理论上的可能性，但我认为OpenAI会继续融资，其他公司也会继续投入，因为一旦实现AGI，回报将是巨大的。<br>LexFridman：所以你认为最终会有多家公司获胜。<br>DylanPatel：我不认为会是赢家通吃的局面。<br>LexFridman：好吧，那我们暂且不提AGI，就说是更强大的AI吧。这不是一蹴而就的事，而是一个渐进的过程。<br>NathanLambert：超级AI。一个不断增强、功能快速增长的集合。有些公司会从AI中受益，但不是因为他们训练出了最好的模型。Meta在其各项服务中都有很多AI应用场景。用户在他们的平台上花时间，AI可以提高每用户每小时的收入。<br>LexFridman：看来Google和特斯拉&#x2F;xAI也非常重要。Meta也能受益，不一定直接通过大语言模型，而是通过AI带来的智能提升现有产品，比如推荐系统。马斯克一直在谈论Optimus机器人，机器人智能化也是一个方向。他认为机器人业务的规模将超过10万亿美元。<br>NathanLambert：长期来看，也许有可能，但短期内不太现实。谁知道机器人技术何时才能成熟应用？<br>DylanPatel：我们来做个简单的市场规模分析：全球80亿人口，假设人均一个机器人，按平均工资计算，市场规模就能超过10万亿美元。<br>LexFridman：如果机器人普及到各行各业，为什么只能有80亿个机器人？我已经看到了机器人的应用前景。所以，关键在于他们销售的产品。这就是为什么OpenAI的处境更微妙，因为——<br>NathanLambert：目前OpenAI的品牌价值几乎都集中在ChatGPT上。对大多数用户来说，他们没必要非得用OpenAI最新的、花费数十亿美元的模型，授权使用未来的Llama5可能成本更低，效果也足够好。ChatGPT对OpenAI来说是一个非常有价值的资产，但他们可能需要探索更多盈利模式，而不是仅仅依赖ChatGPT。<br>DylanPatel：聊天应用的增长空间确实有限。用户用它来问些随机问题，成本持续下降。<br>NathanLambert：未来可能会靠广告支撑。<br>DylanPatel：对，可能会有广告。Meta已经服务了405B用户，可能还在亏损，但模型成本会持续降低，最终他们可以免费提供广告支持的模型，Google也有能力做到这一点，而且他们的用户覆盖面更广。聊天不会是唯一的应用场景。推理、代码生成、AI智能体、计算机使用等等，这些都是OpenAI未来必须涉足的领域，才能实现盈利，否则他们可能会被淘汰。<br>LexFridman：但X、Google和Meta都有其他产品线。那么OpenAI和Anthropic最终可能会被边缘化甚至消失吗？<br>DylanPatel：除非他们的模型能一直保持领先，而他们目前确实做得不错。<br>LexFridman：但这始终是一个非常前沿、变化迅速的领域。<br>NathanLambert：这取决于你对人工智能未来发展方向的判断。<br>LexFridman：必须持续保持领先。即使人工智能正朝着AGI飞速发展，也需要不断攀登高峰，保持领先地位。X在数据方面有优势，Google也是，Meta在数据、其他产品和资金方面都更有优势。<br>DylanPatel：现在的共识是，人类数据已经被充分挖掘了。大家不再关注这一点，而是转向自对弈、可验证的任务。<br>LexFridman：是的，自对弈的关键在于随机数生成器(RNG)的质量。<br>NathanLambert：AWS有戏不？AWS并不是在每台服务器上赚很多钱。最强大的人工智能平台也是如此，即使API调用非常廉价，拥有平台本身就能带来巨额收入。很多人认为AI平台是下一代计算层。<br>DylanPatel：你必须相信这一点。很多人认为token和token经济学，以及大语言模型API将成为下一代计算层，是新的经济范式，就像能源和石油一样。但前提是你必须相信API和聊天应用不是人工智能的终点。任务型AI、AI智能体、机器人技术和计算机使用才是价值所在，而不是API或聊天应用。<br>LexFridman：那么，有没有可能最终一切都变成商品化，只剩下Perplexity这种套壳公司——我无意冒犯哈。<br>NathanLambert：现在倒是真的有很多套壳公司赚了很多钱。<br>LexFridman：但你认为人们最终可能会忘记OpenAI和Anthropic的存在吗？只剩下围绕API的各种套壳应用，并且可以动态切换底层模型…<br>DylanPatel：如果模型进步停滞，那是有可能的。AI正在商品化。DeepSeekV3就证明了这一点。Llama3B的成本比GPT-3低1200倍。任何基于GPT-3或GPT-4级别能力构建的商业模式都岌岌可危。<br>NathanLambert：现在最成功的企业，往往是那些能随着模型性能提升而不断迭代升级的公司。<br>LexFridman：没错。就像套壳应用，它们搭上了模型进步的浪潮。<br>NathanLambert：短期内，最赚钱的公司可能是那些能找到将大语言模型应用于广告投放的公司。比如Meta的信息流广告，可以精准定位用户，而不是像传统广告那样投放给特定内容。Google的搜索广告，亚马逊的搜索广告也在快速增长。但在ChatGPT的回复中，如何植入高质量广告仍然不清楚。如果能在模型成本持续下降的同时解决广告问题，就能获得超高收益，这部分收入潜力巨大，但技术上如何实现尚不明确。<br>LexFridman：就像Google的AdSense创新一样，也许有一天我们会在GPT输出中看到广告，这将带来数十亿甚至数万亿美元的收入。<br>NathanLambert：这种广告形式可以非常巧妙，可以融入对话本身，比如现在的语音模式，可以以某种方式在语音交互中加入广告。这更难衡量和设计，需要想象力。<br>LexFridman：而且广告不能显得突兀，不能引发用户反感，需要平衡好广告的清晰度和用户体验。这是Anthropic和OpenAI正在努力解决的难题。<br>NathanLambert：他们可能并没有把重心放在广告盈利上。<br>DylanPatel：我认为他们根本不关心广告。<br>NathanLambert：Perplexity这样的公司可能在做更多相关尝试。<br>DylanPatel：Perplexity、Google、Meta都在关注广告盈利，而OpenAI和Anthropic完全专注于AGI。他们认为，只要构建出AGI，就能赚到足够的钱，或者至少能覆盖所有成本。所以说，对AGI的预测时间很大程度上决定了这些公司的战略布局。<br>AI智能体<br>LexFridman：你们认为AI智能体（Agent）有发展前景吗？这是近两年的热门话题，很多商界人士都在用这个时髦的词，说AI智能体将彻底颠覆一切。<br>NathanLambert：“智能体”这个词显然被过度炒作了。我们之前讨论过强化学习，它可以训练模型产出可验证的结果。“智能体”本应是指能够自主、开放式地解决任务，并适应不确定性的系统。但现在很多“智能体”应用，比如苹果的AppleIntelligence——虽然我们还没见到真机，但发布会上说它可以协调不同应用，实现跨应用协同，这种工具应用正是大语言模型擅长的——而它本质上是一个封闭系统，在后台通过AI将你的信息应用与照片等数据整合。很多软件公司为了蹭热度，也把自己产品描述成“智能体”。<br>关键问题是，如何让大语言模型扩展到新领域，并实时自主解决问题？也许可以通过自我微调或上下文学习来实现，上下文学习指的是将信息存储在提示词中。你可以用学习算法来更新提示词，但问题是，你是否真的相信AI智能体可以可靠地完成复杂任务，比如“帮我预订两天后去奥斯汀的行程，我有以下XYZ限制条件”？我认为这里存在人机交互和信任问题。<br>LexFridman：你们对此有何预测？我的直觉是，我们离真正实用的AI智能体还很远。<br>DylanPatel：OpenAI提出了一个AI发展阶段论，聊天是第一阶段，推理是第二阶段，智能体是第三阶段。后面还有两个阶段。值得注意的是，我们在聊天阶段停留了好几年，现在刚进入推理阶段，估计还要停留一两年才能到智能体阶段。当然，与此同时，人们可以尝试模拟下一阶段的能力。智能体可以自主完成任务，持续几分钟甚至几小时。推理还只能一次处理几十秒的任务，返回的结果还需要人工验证。<br>最大的挑战在于，这和制造业面临的问题类似。制造业有一个叫六西格玛（SixSigma）的概念，追求极致的质量和可靠性。半导体制造有成千上万个步骤，即使每个步骤的良品率达到99.99999%，最终总良品率可能只有60%，甚至更低。<br>所以智能体也是一样，需要将多个任务串联起来，每一步都不能出错。但即使是最好的大语言模型，在基准测试中也无法达到100%准确率，总会有些许误差。这和自动驾驶面临的挑战类似。自动驾驶之所以难以实现，就是因为可靠性不够高，即使像GoogleWaymo那样在特定区域进行“地理围栏”，也需要远程操作员随时待命，处理突发情况。<br>LexFridman：自动驾驶还有很多结构化约束，比如道路规则、交通法规等等。但开放网络或开放式计算机操作环境则是一团糟。可能性太多了。我一直对任何需要与人类世界、开放信息世界互动的系统持怀疑态度。<br>NathanLambert：确实如此。如果AI智能体无法完全自主可靠地处理复杂的人类世界，我们可以像Waymo那样，建立人工操作员团队，支持某些特定的工作流程。<br>DylanPatel：有家公司，我忘了名字，他们的宣传语就是“AI智能体失灵时，我们的人工操作员顶上，一个电话就能搞定！”就像API调用一样，挺搞笑的。<br>NathanLambert：等人形机器人普及后，可能会出现远程操作市场。总会有人愿意远程解决机器人无法完成的任务，比如“洗碗机装载得不满意，远程帮我调整一下”。这也许会成为一个全新的外包行业。<br>LexFridman：我甚至能想象，一个AI智能体和另一个AI智能体对话，一家外包公司专门提供AI智能体，帮助其他AI智能体解决问题。<br>NathanLambert：但如果你能做出擅长某一步骤的AI组件，就可以把它们组合起来。这就是为什么即使AI智能体成熟还需要很长时间，我们也要提前构建基础设施来支持它。现在已经出现了一些先行者，他们与特定网站、DoorDash、OpenTable等建立了合作关系，这些合作关系能帮助他们快速提升能力。他们的模型在这些特定领域会变得非常出色。这可以看作是概念验证，也可能形成网络效应，吸引更多公司让人工智能更容易使用。当然，也有些公司会设置壁垒，就像我们看到的互联网发展史一样，现在大语言模型的训练数据也面临类似的问题，有些公司开始收费。这就是商业规律。<br>LexFridman：话虽如此，航空公司和酒店其实有很强的动力把网站用户体验做好，但他们通常做不到。预订机票的流程繁琐得令人发指。<br>NathanLambert：你现在甚至没法给美国航空客服打电话了，他们根本不提供电话号码。<br>LexFridman：他们的网站界面简直是灾难。我作为人类用户都觉得难以忍受，真不知道AI智能体怎么才能搞定那些网站。每次预订机票都像经历一场生存危机。我认为构建一个能稳健处理这类网站的AI智能体极其困难。<br>NathanLambert：但你想想，美联航已经接受了Starlink的服务条款，这意味着他们必须免费提供Starlink，用户肯定会喜欢。如果有一家航空公司说，“我们将花一年时间，让我们的网站对AI智能体完美适配”，每次有人用AI预订机票，都会首选这家航空公司。<br>DylanPatel：他们可以直接说，“我们提供API接口，只对AI智能体开放。通过API查询机票，价格统一上浮10%，但保证你能查到我们所有航班，随便预订。”<br>NathanLambert：那就成了。<br>DylanPatel：就像“我们提价10%，就这么定了。”用户是否愿意为了“帮我订一张去XX的机票”这样的便利而接受10%的溢价？我觉得很多人会愿意。开放世界和现实世界确实非常混乱，但如果将问题限定在狭窄的范围内，人们就能创造出非常有价值的应用，并大幅降低成本。像家用机器人这种过于宽泛的应用，会像自动驾驶一样面临重重挑战，因为失败模式太多了。但如果限定在特定网站集和任务集，比如导航特定网站、执行特定操作，或者给冰箱拍照，上传菜谱，然后AI自动计算出需要从亚马逊或全食超市订购哪些食材，这些应用场景其实很快就能实现。这将催生一系列商业机会，很多人对从中盈利持乐观态度。<br>NathanLambert：需要明确的是，这些沙盒环境在研究领域已经存在了。有人已经构建了所有主流网站的克隆版本，比如Google、亚马逊等等，OpenAI内部可能也有类似的沙盒，用于训练AI智能体。这和DeepMind的机器人团队многолет前就拥有的机器人集群类似，你可以在完全隔离的环境中远程控制机器人。他们在伦敦有个实验室，你可以远程发送指令，让机器人堆积木，进行各种实验。当然，实验室里有技术人员负责维护和修理机器人。我们раньше就已经开始探索自动化了。<br>我认为，AI智能体的发展路径会是从沙盒环境起步，逐步扩展到更开放的领域。自然语言处理和语言模型的发展历史也印证了这一点。早期的大语言模型只能完成单一任务，后来指令微调技术出现，人们开始让模型同时处理越来越多的任务，最终模型开始泛化到各种任务。我们现在正处于AI智能体发展的早期阶段，可能很快就会跨越泛化的临界点。<br>AI编程<br>LexFridman：你们对编程领域怎么看？软件工程是很多人与人工智能互动最频繁的领域。<br>DylanPatel：计算机科学专业的学生现在很焦虑，但软件工程可能是人工智能收入和生产力提升最大的领域。无论是Copilot、Cursor还是ChatGPT，我认识的程序员几乎都在用ChatGPT，很多人还订阅了200美元的付费套餐，因为真的很有用。<br>软件工程领域已经出现了SWE-bench这样的基准测试。对人类来说，这套测试题说难不难，说易倒也不易。我认为要做好SWE-bench，至少需要几年计算机科学学习或编程经验。而模型在一年内，SWE-bench的成绩从4%提升到60%，明年会达到什么水平？肯定更高。可能无法达到100%，但会达到一个很高的水平。到那时，我们需要更难的软件工程基准测试。<br>现在大家普遍认为，AI可以代码补全，这很简单。也能生成一些函数，但需要人工审查。这也不错。但我认为软件工程智能体可能会比其他智能体更快实现，因为这是一个可验证的领域。你可以随时进行单元测试或编译。而且AI可以一次检查整个代码库，这是人类工程师做不到的，只有架构师或资深人士才能从宏观层面考虑这些问题，定义框架，然后智能体可以执行。所以我认为软件工程的成本会大幅下降。<br>成本降低后，市场也会发生变化。在美国，平台型SaaS公司很多，比如Salesforce。但在中国，平台型SaaS并不流行，大家倾向于自建技术栈，因为中国的软件工程成本低得多，STEM毕业生数量庞大等等。自建更划算。<br>同时，代码大语言模型在中国的普及率远低于美国，因为中国工程师的成本更低。但如果每家公司都能以极低的成本快速构建自己的业务逻辑，会发生什么？大家会减少使用平台型SaaS，转而构建定制化解决方案，并快速迭代。业务效率也会更高，因为无需迁就那些不够灵活的平台型SaaS，或者处理那些原本不需要AI的自动化流程。<br>很多逻辑其实一直存在，只是缺乏构建的动力。AI可以加速这一切。软件领域之外，工业、化学、机械工程师通常不擅长编程。他们的工具，比如半导体工程师的工具，还停留在20年前。包括ASML光刻机在内的很多工具都在WindowsXP上运行。很多分析工作还在用Excel完成。他们完全可以利用现有数据和技术，提升效率。需要将软件工程技能普及到各个领域的专业工程师手中。我认为这是通用人工智能创造价值的巨大潜力所在。<br>NathanLambert：总的来说，软件工程领域不会出现断崖式衰落。MetaStories功能的例子就很好地说明了增长模式的变化。SnapchatStories功能推出后，增长势头戛然而止。软件工程师的需求，在AI冲击下，可能只是增长停滞，不会出现大规模失业。因为人才供给调整速度较慢，学生数量还在增加，岗位需求调整会滞后，可能要过几年才会显现。长期来看，比如20年、40年后，软件工程师的需求可能会大幅下降。但在未来几年，软件工程师不会突然变得毫无用处。<br>LexFridman：我认为程序员的工作性质和内容会发生变化。很多环节仍然需要人参与。人类在代码审查、修复bug等方面仍然扮演着重要角色。<br>DylanPatel：以及进行超出上下文长度的思考。<br>LexFridman：还有调试，比如阅读代码来理解系统逻辑。在提示词中加入更多信息，加入人工干预等等。<br>NathanLambert：以及设计完美的Google按钮。Google以拥有能设计出完美按钮的工程师而自豪。AI能做到吗？也许AI可以提供很多设计方案，但最终还需要人来评判。<br>LexFridman：这就是关键。我们其实可以称之为“品味”。人类比AI系统更擅长判断其他人喜欢什么。这涉及到偏好学习。人类是最大的偏好生成器，偏好就来源于人类。<br>NathanLambert：人类确实擅长在多个选项中进行比较和判断。这和RLHF以及偏好微调的核心思想一致——很多问题很难直接生成完美答案，但很容易判断哪个答案更好。这就是我们现在用人类来训练AI的方式，让人类评判哪个输出更好。这可能也是软件工程的未来模式，即代码审查。AI提供多个代码选项，列出优缺点，人类工程师负责评判和选择。<br>LexFridman：我强烈建议程序员开始拥抱AI，接受监督和协作的角色，而不是抵触AI或完全依赖AI生成代码。我认为，要管理日益智能的系统，程序员需要具备更高水平的专业知识。<br>DylanPatel：没错，还要成为某个领域的专家。如果你看看航空航天、半导体或化学工程等领域，会发现大家都在用非常老旧的平台和软件。很多时候，数据科学家成了笑话。但实际上，应该将AI前沿技术应用到各个领域，提升各行各业的生产力。即使前沿技术来自人工智能，你身处行业前沿，就能充分利用AI浪潮。<br>开源<br>LexFridman：我们还有一个话题没深入聊，就是开源。首先，祝贺Nathan的实验室发布了新模型Tülu。<br>NathanLambert：我来解释下Tülu的由来。“Tülu”这个词指的是单峰骆驼和双峰骆驼杂交的混血骆驼。ChatGPT刚兴起时，涌现出一批以动物命名的模型，比如Alpaca（羊驼）、Vicuna（小羊驼）等等。Tülu这个名字由来已久，也属于这个命名体系。<br>我们一直在利用开源代码进行后训练研究。这次发布的Tülu模型，第一部分在2024年秋季就发布了，我们基于Llama的开放模型和开放权重模型，加入了我们完全开源的代码和数据。有个流行的基准测试叫ChatbotArena，通常用来评估聊天模型的性能，它让人类盲测不同机构的模型。如果你看过11月或12月的排行榜，前60名模型来自几十个机构，但其中没有一个模型公开了后训练的代码或数据。<br>更不用说预训练数据和代码了，几乎没有开源的。后训练相对容易实现，成本也较低。关键问题是，在代码和数据完全开放的情况下，模型性能可以提升到什么程度？这是Tülu项目的初衷。我们借鉴了Llama和英伟达Nemotron模型的经验，Nemotron的后训练方案相对开放，包含一些数据和论文。我们将这些经验结合起来，试图创建一个配方，让大家可以微调模型，使其在特定领域达到类似GPT-4的水平。<br>LexFridman：为了更清晰地说明，以Tülu为例，也许可以顺便谈谈Llama。Tülu是基于Llama3405B做的吗？<br>NathanLambert：Tülu是一系列后训练方案，我们多年来做了多个模型。<br>LexFridman：你们把所有东西都开源了？<br>NathanLambert：对。开放权重的模型有一个典型，也就是Llama，但Llama严格来说不算完全开源，因为你不知道Meta在里面放了什么。所以我们做了个独立的项目Olmo，稍后会讲到。<br>Tülu的重点是让用户可以自由扩展和定制后训练流程。我经常听到初创公司和企业说，“我们可以用Tülu的后训练方案，应用到我们自己的领域。”我们一直在研究验证器（verifiers），采用了可验证奖励的强化学习RLVR思想，类似于RLHF。我们将其应用于MATH基准测试，以及今天发布的模型，也就是去年的Llama405B基础模型。我们还有其他技术，比如指令微调和偏好微调。但有趣的是数学方面，在数学基准测试上提升模型性能相对容易。有个叫MATH的基准测试，评估模型的数学能力，名字很难记，因为我们是研究人员，不是品牌战略家。<br>DeepSeek的论文也提到，在更大的模型中，更容易通过RL训练激发强大能力，然后再将这些能力从大模型蒸馏到小模型。我们今天发布的Tülu模型也看到了类似现象。我们在AI2的计算资源有限，无法训练405B模型，所以只跑了几次，效果就很好。这表明开源后训练还有很大潜力。<br>DylanPatel：而且Tülu的性能超过了Llama官方发布的版本，比Llama更好。<br>NathanLambert：是的。我们的评测数据，当然我们有几个月的先发优势，但Tülu的评测数据确实比Llama发布的指令模型要好得多。<br>LexFridman：你好像还说比DeepSeekV3好？<br>NathanLambert：在我们的评测基准上，DeepSeekV3和Tülu很接近。我们加了个安全基准，评估模型是否会输出有害内容。Tülu的安全评测得分较低，拉低了总分。<br>DylanPatel：你们的评测基准是多个指标的综合，还是指某个特定指标？<br>NathanLambert：我们用包含10个评估项的评测套件，这是后训练的通用做法，选择你关心的评估指标。学术界或小型实验室，评估指标会少一些。大公司会有自己特别关注的领域，像FrontierLabs可能会有几十甚至上百个细分指标。我们选择了一套有代表性的评估套件，包括聊天能力、指令跟随能力（比如只用表情符号回复，让模型执行奇怪的指令）、数学能力、代码生成能力，以及安全性。<br>安全性是评估套件的10个指标之一，反映了AI社区普遍关注的问题。比如，Tülu的平均评测得分是80分（包含安全性），DeepSeek在不考虑安全性的情况下平均得分是79%，但安全评分会把DeepSeek的总分拉低到70左右。<br>DylanPatel：所以即使不考虑安全性，你们也能胜过DeepSeek。<br>NathanLambert：这只是内部评测，我不想为了“赢”而刻意调整评测基准。有些人可能不在意模型安全性，安全性可以在下游应用中解决，比如在API托管时加入安全措施。如果你想标榜自己的方案是最好的，就不能因为某些人可能不需要的指标而限制模型性能。<br>而且，技术进步日新月异，我们晚发布模型，反而有更多时间学习新技术，比如RL技术。我们秋季就开始做Tülu项目，现在推理模型非常流行。开源后训练的下一步是扩大验证器规模，扩充数据量，争取复现DeepSeek的一些成果。DeepSeek的论文很有参考价值，让我们少走了很多弯路。这就是学术界和前沿AI研究的现状：开放合作，共同进步。<br>LexFridman：既然你在推动开源，你认为开源的未来是什么？DeepSeek开源或开放权重，是否改变了游戏规则，推动了开源运动？<br>NathanLambert：这要回到许可证的问题。DeepSeekR1的商业友好型许可证，确实是一次重大突破。这是我们第一次看到真正的前沿模型，开放权重，而且许可证对商业应用非常友好，对下游用例没有任何限制，比如数据蒸馏等等。在ChatGPT出现以来的AI发展史上，从未有过先例。之前的开源模型，要么性能落后，要么许可证很奇怪，无法真正商用。<br>DylanPatel：Meta的Llama许可证，除了限制五家大公司，几乎等同于开放许可？<br>NathanLambert：这涉及到“什么是开源AI”的定义。Llama许可证对用例有限制，比如不能用于特定用途。如果你有开源软件背景，会认为这不算真正的开源许可证。<br>DylanPatel：具体有哪些限制？<br>LexFridman：像竞争对手之类的限制？<br>NathanLambert：以前有个限制是不能用于军事用途，后来为了扩大应用范围取消了。现在可能还有禁止用于儿童色情等用途的限制。但这足以让开源社区的人认为Llama不是真正的开源许可证。<br>而且Llama许可证还有个很奇怪的条款，如果你使用了Llama模型，你的模型也必须命名为Llama或包含Llama字样。这涉及到品牌问题。如果一家公司用了Llama，严格来说，许可证要求他们在应用底部注明“PoweredbyLlama”。从营销角度来看，这显然不利。作为研究人员，我还能接受，觉得“没关系，标一下就标一下”。所以我们这次发布的所有资料都标着Llama-dash。但我们需要的是真正开放的模型。<br>DylanPatel：等等，你的意思是，我不能廉价复制Llama，然后冒充是自己的模型，但我可以用中国的DeepSeek这样做？<br>NathanLambert：当然可以。这就是我想说的。这就是为什么我们需要完全开放的大语言模型。Olmo的目标就是尽可能做出所有环节都开放，性能又接近前沿的模型。当然，我们受到计算资源和人员的限制。我们请教JohnSchulman这样的大牛，从URL和输出中获取insights，才能实现性能的飞跃。但要推动开源AI达到前沿水平，还需要很长时间。<br>我认为开源AI缺乏像开源软件那样有效的反馈循环。开源软件强调安全性和可复用性。开源软件有很多优点，比如安全、可复用，加入新公司后也能继续使用开源工具。但开源大语言模型，即使你公开了数据和训练代码，别人也很难在此基础上构建和改进，因为需要大量的计算资源和专业知识。<br>在开源AI建立起有效的反馈循环之前，开源似乎更多是一种理想主义的追求。我们需要思考如何利用开源AI，建立一个围绕“公开大语言模型数据有什么好处”的生态系统。这方面还没有太多成熟的模式。我们计划尽快推出一个演示，用户可以查询Olmo模型，查看哪些预训练数据与查询内容相似。这在法律上有些风险和复杂性，但这有助于大家理解“AI训练数据公开意味着什么”。训练数据动辄数TB，内容浩如烟海，我也不知道里面有什么。但如果想让开源AI在商业上可行，就必须建立起这样的生态系统。<br>展望未来<br>LexFridman：展望未来几年，无论是集群建设还是AI技术突破，你们有什么期待？未来2-4年，你们能想象到的最佳图景是什么样的？可以是具体的、技术性的，比如后训练的突破，或者只是规模上的飞跃，比如更庞大的集群。<br>DylanPatel：我个人很喜欢追踪供应链，关注参与者和他们的分工。数字、成本、产能建设、如何帮助企业规划产能、赢得订单、制定战略，这些都让我觉得很有意思。技术层面，光学器件和电子器件的融合趋势让我兴奋，无论是共封装光学，还是新型交换技术，网络领域有很多值得期待的创新。<br>LexFridman：你指的是集群内部的网络？<br>DylanPatel：是的，还有多数据中心训练。人们在数据中心之间铺设大量光纤，并用高带宽点亮，这方面也有很多有趣的可能性。5G之后，电信领域一直比较沉闷，现在硬件层面又开始变得刺激起来了。<br>LexFridman：你能科普一下速度的概念吗？比如内存速度、互连速度，以及数据中心之间光纤的速度，这些速度量级差异大吗？未来有可能统一到一个水平，让整个集群像一台计算机一样吗？<br>DylanPatel：不可能。我认为编程只会越来越难，越来越复杂，层次会越来越多。大家常用内存层次结构来类比，芯片内部有寄存器，速度最快，距离计算单元最近；然后是缓存，速度稍慢，在更多计算单元之间共享；再往外是HBM或DDRR之类的DRAM内存，速度更慢，在整个芯片之间共享；再往外是内存池，在多个芯片之间共享；最后是存储。跨数据中心、数据中心内部、芯片内部的访问延迟差异很大，所以需要不同的编程范式来适配，编程会越来越难。当然，AI也许能帮助编程。<br>你想想，任务复杂度越高，扩展性就越差。芯片数量翻倍，性能不可能翻倍，这就是计算的现实，因为总会有低效环节。大家都在努力提升扩展性，比如让芯片更紧密互联，采用更先进的编程模型，或者在模型算法层面进行创新。DeepSeek在互连受限的情况下，仍然实现了并行计算，就有很多创新之处。谷歌等公司也在做大量相关研究。模型、工作负载和技术创新都非常令人兴奋。硬件层面，固态变压器很有意思。电源和电池技术，以及各种散热技术，也都在快速发展。<br>如果你审视计算堆栈的每一层，从光刻、蚀刻到制造，到光学器件，到网络，到电源，到变压器，到冷却，到网络，再到数据中心空调，甚至铜缆都在创新。铜缆的封装密度也在不断提升。整个堆栈的进步速度前所未有，一直到最顶层的模型层面。<br>LexFridman：我能想象你坐在一个堆满屏幕的房间里，监控着所有集群的供应链信息，简直太厉害了。<br>DylanPatel：我们有个庞大的团队，分工协作。<br>LexFridman：你们在半导体分析方面做得非常出色，把握住了数字世界和人类文明的脉搏，这感觉很酷，像是感受我们所有人共同创造的史诗。Nathan，你对未来的技术突破有什么期待？<br>NathanLambert：听完Dylan精彩的分析，我思考了一会儿。<br>DylanPatel：他被我的话语给迷住了。<br>NathanLambert：我觉得训练模型真的很有趣，因为进步空间巨大，容易取得进展。我喜欢训练模型，分析模型背后的原理，这本身就很有趣，因为显然还有很多未知领域。我公开分享研究成果，是希望更多人参与到AI发展中来，而不是让少数人垄断AI话语权，宣称“相信我们，我们会让人工智能造福人类”。<br>我们需要更多人了解AI，理解AI，参与AI发展。这不完全是积极的愿景，但AI注定会成为影响我们一生最重要的技术，我们需要让更多人参与进来，并且在我看来，更开放的环境更有利于AI生态发展，让更多人了解AI的现状，无论是其他领域的研究人员，还是政府，还是公众。当然，开放并非万能药。未来我们需要重新评估AI面临的最大挑战，用不同的视角应对这场技术变革。<br>LexFridman：从用户体验角度来看，AI也展现出令人惊艳的潜力。就像所谓的“顿悟时刻”（AhaMoments），AI展现出的推理能力和思维链，蕴含着一种原始的美感，仿佛一面镜子，让我们看到人类智慧的独特之处。正如那些科技公司的宏大愿景，AI正在触及“智能”的本质，让我们意识到人类智慧的珍贵，以及意识的神秘，而这些谜团正是我们未来可以探索的方向。我从没想过AI能发展到今天这种程度。当年我兴奋地观看“深蓝”击败卡斯帕罗夫，也从未预料到AI会在我这辈子变得如此强大。这才是真正的AI。<br>NathanLambert：我刚开始接触AI时，是训练一个很笨的四旋翼飞行器，让它学会飞行，然后向上飞，撞到天花板，停下来，然后接住自己。和现在AI的进步相比，那真是小儿科。<br>LexFridman：现在你可能只需用自然语言告诉AI“学习飞行”，它就能自动生成控制算法。<br>NathanLambert：当然，底层技术还有些障碍需要克服，但基本可以实现。<br>LexFridman：回到我们之前讨论的机器人话题，AI在虚拟世界表现出色，但在现实物理世界中与物体互动仍然很难。展望未来10年、100年、1000年，是什么让你们对人类文明的未来充满希望？你们认为人类文明还能延续1000年吗？<br>NathanLambert：我认为1000年后人类肯定还会存在，大概率吧。当然，有很多潜在的灾难可能发生，人类数量可能会锐减，但人类非常擅长生存，历史已经无数次证明了这一点。我们可能不擅长预测长期风险，但当风险迫在眉睫时，我们总能找到应对之策。<br>因此，我对AI失控接管人类之类的“奇点”并不担心，因为物理规律和人类解决问题的能力会限制这种情况发生。当然，国际局势等其他问题令人担忧。但人性本善，我们也在努力放大善的一面。我认为我们正处于一个动荡时期。从整个人类历史来看，有进步，也有倒退，也有停滞不前。而我们现在正走在一个积极的轨道上。<br>LexFridman：是的，似乎整体在进步，但就像权力一样，人类的苦难也会周期性爆发，我们希望尽可能减少苦难的峰值。<br>DylanPatel：总的来说，人类的苦难会大大减少，我对未来非常乐观。但我确实担心技术法西斯主义之类的风险。随着AI越来越普及和强大，掌握AI的人可以做更多事情。也许AI不会毁灭全人类，但可能会出现少数精英阶层，他们通过脑机接口等技术，与通用人工智能深度融合，从而拥有远超常人的能力，统治世界和经济。<br>我认为人机融合更令人担忧。它可能使个别人类对世界产生更大的影响，既有积极影响，也有消极影响。总的来说，人类对世界产生了积极影响，至少在社会层面是这样，但个别人类也可能造成极端破坏。<br>而通用人工智能，至少在我理解的实验室定义中，并非失控的实体，而只是一种能高效完成多项任务的工具，它放大了个人造成极端破坏的能力。但在大多数情况下，我认为AI会被用于追求利润，增加社会财富和物质供应，从而减少人类的苦难。这才是最终目标。<br>LexFridman：比如像现在刷着短视频，沉浸在多巴胺的快乐中……<br>DylanPatel：在信息流中保持开放和静止的状态。<br>NathanLambert：在信息流中保持世界现状。<br>DylanPatel：这也是一种积极的结果，对吧？如果给我提供营养液，让我躺着刷手机，我也会很开心的，这也是一种幸福。<br>LexFridman：这真是一个有趣的时代。感谢你们推动人类潜力的边界。最后，我想用理查德·费曼的一句话来结束今天的对话——“对于一项成功的技术，现实必须优先于公关，因为大自然是不可欺骗的。”<br>当“人工”智能还依赖着海量“人工”标注数据时，一个完全由强化学习（RL）训练出的推理模型——DeepSeekR1，如同一颗石子投入平静的湖面，激起层层涟漪。<br>没有人类反馈（NoHumanFeedback），R1仅凭纯粹的RL，便在推理能力上实现了惊人突破。这不禁让人想起AlphaGo的“第37步”（Move37，本文其实也有提到），那超越人类理解的一步，是否预示着AI领域也将迎来属于自己的“神之一手”？<br>强化学习，这个一度被认为更适用于游戏和控制领域的“老古董”，如今却在大模型领域焕发新生。它与模仿学习的本质区别究竟在哪里？AndrejKarpathy所说的“模仿学习是第一种方式，强化学习是第二种方式，也是魔力来源”，究竟蕴含着怎样的深意？<br>当然，纯RL训练并非一片坦途。奖励函数的设计、探索空间的爆炸、模型安全与价值观的对齐……这些难题如同悬在头顶的达摩克利斯之剑，时刻考验着研究者们的智慧。<br>DeepSeek的这次尝试，无疑为中国AI产业注入了一针强心剂。但如果大家都选择跟进，未来的竞争差异又将体现在哪里？是算力？是算法？还是……？<br>2月7日晚8点至9点半，CSDN视频号直播间，《DeepSeek暨AI进化论十日谈》第二期，我们将聚焦“强化学习是否会带来大模型的范式转换？”这一核心议题，与您一同拨开迷雾，探寻真相。欢迎关注预约。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ZejunCao</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://zejuncao.github.io/2025/02/07/1000002540-2247585524-1/">https://zejuncao.github.io/2025/02/07/1000002540-2247585524-1/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">ZejunCao</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/">
                                    <span class="chip bg-color">开源项目</span>
                                </a>
                            
                                <a href="/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E8%81%9A%E5%90%88%E5%B9%B3%E5%8F%B0/">
                                    <span class="chip bg-color">微信公众号聚合平台</span>
                                </a>
                            
                                <a href="/tags/AI%E7%A7%91%E6%8A%80%E5%A4%A7%E6%9C%AC%E8%90%A5/">
                                    <span class="chip bg-color">AI科技大本营</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/02/07/1000000424-2247492781-1/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/1000000424_2247492781_1.jpg" class="responsive-img" alt="仅817样本超越o1-preview，上交大LIMO&#39;少即是多&#39;推理新范式">
                        
                        <span class="card-title">仅817样本超越o1-preview，上交大LIMO&#39;少即是多&#39;推理新范式</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/">
                        <span class="chip bg-color">开源项目</span>
                    </a>
                    
                    <a href="/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E8%81%9A%E5%90%88%E5%B9%B3%E5%8F%B0/">
                        <span class="chip bg-color">微信公众号聚合平台</span>
                    </a>
                    
                    <a href="/tags/PaperAgent/">
                        <span class="chip bg-color">PaperAgent</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/02/07/2650954015-2650954015-1/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/2650954015_2650954015_1.jpg" class="responsive-img" alt="历史时刻：DeepSeek GitHub星数超越OpenAI，仅用时两个月">
                        
                        <span class="card-title">历史时刻：DeepSeek GitHub星数超越OpenAI，仅用时两个月</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-02-07
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/">
                        <span class="chip bg-color">开源项目</span>
                    </a>
                    
                    <a href="/tags/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E8%81%9A%E5%90%88%E5%B9%B3%E5%8F%B0/">
                        <span class="chip bg-color">微信公众号聚合平台</span>
                    </a>
                    
                    <a href="/tags/%E6%9C%BA%E5%99%A8%E4%B9%8B%E5%BF%83/">
                        <span class="chip bg-color">机器之心</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <a href="/about" target="_blank">ZejunCao</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ZejunCao" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:caozejun369@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1378463428" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1378463428" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/5915009280" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/5915009280" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>





</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
