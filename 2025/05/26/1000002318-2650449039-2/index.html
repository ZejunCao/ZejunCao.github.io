<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="万字长文总结！Reasoning模型的强化学习实现路径, ZejunCao&#39;Blogs">
    <meta name="description" content="万字长文总结！Reasoning模型的强化学习实现路径

仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接

作者：zss123(知乎)https :&amp;#x2F;&amp;#x2F;zhuanlan. zhihu.com&amp;#x2F;">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>万字长文总结！Reasoning模型的强化学习实现路径 | ZejunCao&#39;Blogs</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<meta name="generator" content="Hexo 7.3.0"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">ZejunCao&#39;Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">ZejunCao&#39;Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/frontcover/1000002318_2650449039_2.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">万字长文总结！Reasoning模型的强化学习实现路径</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/AINLP/">
                                <span class="chip bg-color">AINLP</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2025-05-26
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/hC86OUNZm5cSVITPiVNW_A">万字长文总结！Reasoning模型的强化学习实现路径</a></p>
<blockquote>
<p>仅用于站内搜索，没有排版格式，具体信息请跳转上方微信公众号内链接</p>
</blockquote>
<p>作者：zss123(知乎)https :&#x2F;&#x2F;zhuanlan. zhihu.com&#x2F;p&#x2F;1906311849588294687<br>Reasoning模型正在兴起，近期也引发了大量的研究。本文主要是总结近期reasoning模型的强化学习实现路径，给相关领域带来一些参考。<br>本章节将深入探讨强化学习训练的基础方面，这些方法论在各种应用中均有所体现，即使在不明确涉及外部工具的场景下也是如此。然而，值得注意的是，许多现代大型语言模型的强化学习应用本质上都涉及某种形式的“工具”，例如代码执行环境，即便它不是一个外部应用程序接口(API)。<br>数据在任何机器学习范式中都扮演着至关重要的角色，强化学习也不例外。高质量、相关性强的数据是训练出高性能智能体的基石。<br>强化学习训练数据的选择越来越强调质量和相关性，而非单纯追求数量。研究表明，从多样化且与目标任务紧密相关的领域获取数据至关重要。例如，在数学推理任务中，研究者倾向于使用如OpenThoughts、NuminaMATH、MATH以及DeepScaleR等数据集。对于更广泛的问答任务，NaturalQuestions(NQ)、TriviaQA、HotpotQA和SQuAD等数据集是常见的选择。而在如CUDA内核生成这样的专业任务中，则会采用像KernelBench这样的特定数据集。<br>选择可验证的问题或任务是一个关键策略，这极大地便利了后续奖励函数的定义和计算。此外，平衡数据集的难度分布和多样性也受到重视。例如，TORL框架采用LIMR技术来提取具有均衡难度分布的高质量样本，而DeepResearcher则通过调整不同数据集的比例来侧重于多跳推理场景的训练。<br>为了保证输入给强化学习算法的信号是纯净且有效的，数据清洗和过滤是不可或缺的步骤。<br>严格的验证流程是常用的手段，这通常涉及人类专家和强大的预训练模型（例如ReTool中使用的Deepseek-R1）进行双重验证，以过滤掉无效或低质量的数据。TORL框架会过滤掉基于证明的问题以及那些验证标准模糊的问题。DeepResearcher则会过滤掉具有时效性、高度主观性或潜在有害的问题。<br>防止模型依赖记忆信息而非学习预期技能是一个核心挑战。DeepResearcher实施了“污染检测”机制，通过排除那些基础模型无需搜索工具即可回答的问题，确保智能体学习的是搜索等技能，而非利用数据泄漏。这种策略有效地迫使模型学习使用工具或进行更深层次的推理。<br>格式标准化和验证对于后续强化学习过程的效率和稳定性至关重要。例如，ReTool对其代码集成数据进行格式验证，以确保能够高效地检测计算工具的调用触发器。<br>除了选择和清洗现有数据外，针对强化学习的需求对数据进行增强和特定格式的准备也是常见的做法。<br>对于需要“冷启动”的场景，例如工具集成任务，通常会基于已有的文本推理数据进行增强。ReTool框架使用结构化的提示模板，自动将基于文本的推理数据(Dinit)转换为代码集成的推理数据(DCI)，其中人工计算步骤被替换为相应的代码片段及其解释器的执行结果。<br>为了简化奖励函数的计算，有时会对答案格式进行转换。例如，DAPO-Math-17K数据集将数学问题的答案转换为整数形式，从而简化了基于规则的奖励计算，并最大限度地减少了公式解析器可能引入的错误。这种务实的做法使得复杂的推理任务更易于应用强化学习。<br>数据筛选和准备的细致工作，其意义远不止于简单的数据预处理。这些步骤实际上构成了对学习环境的一种隐性塑造。通过精心挑选例如可验证的问题，剔除模糊不清的内容，或是转化数据格式以简化正确结果的识别（如将答案转为整数，或生成代码集成数据），研究人员在奖励函数发挥作用之前，就已经在引导智能体朝向期望的行为模式发展。确保数据“可验证”意味着奖励机制可以更加可靠；将答案转换为整数则简化了奖励机制，减少了学习信号中潜在的噪声或复杂性。这表明，“强化学习数据工程”正成为一个高度专业化的领域，数据准备不再仅仅是初步步骤，而是强化学习设计中不可或缺的一环，它通过预先调整学习环境来巧妙地影响策略学习。<br>同时，数据策略也体现了对模型“学习捷径”这一问题的积极规避。例如，DeepResearcher中的污染检测机制（过滤掉基础模型无需工具即可回答的问题）以及对可验证、无歧义问题的关注，都反映了一种前瞻性的策略。研究者预见到，作为强大模式匹配器的大型语言模型，如果数据允许，它们会利用任何“捷径”。如果模型能从其参数化知识中直接找到答案，它可能就不会学习使用工具。如果数据未经过此类“捷径”的过滤，强化学习智能体可能会通过简单回忆信息或利用数据集偏见来最大化奖励，而不是学习预期的复杂技能（如多跳推理、工具使用）。这会导致在真正需要该技能的任务上泛化能力差。这突显了大型语言模型强化学习中的一个根本性挑战：确保智能体学习的是过程，而不仅仅是模仿数据中的表面相关性。数据管理是应对这一挑战的第一道防线。<br>下表1概述了不同研究工作中采用的强化学习训练数据策略。<br>强化学习算法是驱动智能体学习的核心。近年来，针对大型语言模型的特性，研究者们在经典算法的基础上进行了诸多改进和创新。<br>近端策略优化(ProximalPolicyOptimization,PPO)是当前大型语言模型强化学习领域应用最为广泛的算法之一。它被许多框架作为基础算法。PPO的目标函数（例如，在ReTool的研究中由公式1给出）旨在优化策略模型，同时通过剪裁重要性采样权重或增加KL散度惩罚项，限制新策略与旧策略之间的差异，从而提高训练的稳定性。<br>组相对策略优化(GroupRelativePolicyOptimization,GRPO)是PPO的一个流行变种，它通常通过对同一提示生成的多个响应的奖励进行归一化来估计优势函数，从而避免了训练一个独立的价值网络(critic)。DAPO算法也以朴素GRPO作为基线进行比较。这种方法可以减少计算开销，尤其适用于大型模型。<br>除了PPO和GRPO，还涌现了一系列针对特定问题或为了提升特定性能而设计的专门化变种：</p>
<p>•Dr. GRPO(GRPODoneRight)是对GRPO的改进，通过移除优势计算中的归一化项，旨在消除响应级长度偏差和问题级难度偏差，从而恢复到使用蒙特卡洛回报估计优势的标准PPO目标。</p>
<p>为了更好地将强化学习应用于大型语言模型，研究者们在核心算法的基础上引入了多项关键技术：</p>
<p>•裁剪策略(ClippingStrategies):PPO的裁剪机制对于维持训练稳定性至关重要。DAPO和VAPO通过“Clip-Higher”技术对此进行了增强，该技术解耦了重要性采样比率的上下裁剪范围，允许对低概率词元进行更大幅度的概率提升，从而鼓励探索。RAGEN的StarPO-S也采用了类似的解耦裁剪策略。<br>•价值函数处理(ValueFunctionHandling):尽管GRPO通常省略了学习的价值函数，但基于PPO的方法如VAPO则投入资源进行稳健的价值模型训练，包括使用价值预训练来减轻初始化偏差。StarPO-S为了提高稳定性也重新引入了基于评论家的基线。<br>•词元级与样本级损失(Token-levelvs. Sample-levelLoss):DAPO和VAPO提倡使用词元级的策略梯度损失。这种方法为训练批次中的所有词元分配统一的权重，解决了在样本级损失中较长序列对损失贡献较小的问题，并防止长样本中不期望的模式（如无意义内容或重复）对损失产生不成比例的低影响。</p>
<p>奖励函数是强化学习中引导智能体行为的核心机制。其设计直接影响学习效率和最终性能。<br>•基于结果的奖励(Outcome-BasedRewards):一种普遍采用的方法是使用简单的、基于规则的准确性奖励。例如，在可验证答案的任务（如数学问题）中，如果最终预测答案与真实答案等价，则奖励为+1，否则为-1或0。</p>
<p>•惩罚项(Penalties):为了抑制不期望的行为，会引入惩罚项。Kevin-32B对使用PyTorch函数或不包含CUDA内核的响应（旨在缓解奖励黑客问题）给予0分奖励。TORL曾研究过代码可执行性惩罚（-0. 5），但发现它并未提升模型性能。DAPO对超过最大生成长度的截断样本应用“软性超长惩罚”。RAGEN对不符合格式的响应进行惩罚。<br>•折扣因子(DiscountFactors):在多轮交互的设置中，折扣因子用于平衡即时奖励和未来奖励的重要性。Kevin-32B在多轮训练中使用了0. 4的折扣因子，其中一个响应的奖励是当前内核及其后续所有内核得分的折扣总和。</p>
<p>在优势估计方面，“有评论家(critic-full)”与“无评论家(critic-less)”方法的选择反映了在简洁性&#x2F;效率与稳定性&#x2F;指导性之间的权衡。<br>GRPO的流行，源于它避免了训练一个独立的价值网络，这简化了实现并减少了计算负担，特别是对于大型语言模型而言，同时训练两个大型模型（行动家和评论家）的成本很高。然而，像VAPO和StarPO-S这样的方法则特意重新引入或改进了评论家。<br>VAPO强调通过“价值预训练”和“解耦GAE”来获得更好的价值估计。StarPO-S则利用评论家基线来稳定训练。一个训练良好的评论家可以显著降低优势估计的方差，从而带来更稳定和高效的策略更新。<br>但是，一个训练不佳或未对齐的评论家可能会阻碍学习。选择哪种方法取决于具体问题、计算预算以及对任务而言无评论家优势估计的稳定性感知。这表明在大型语言模型强化学习的优势估计方面没有一刀切的解决方案。<br>该领域正在积极探索这种权衡，从而催生了混合方法或更鲁棒的评论家训练技术。即使在“无评论家”的范式中，从GRPO到Dr. GRPO的演进也显示了基线估计方法的改进。<br>缓解奖励黑客(rewardhacking)是一场持续的“军备竞赛”，需要多方面的解决方案。多项研究都承认并解决了奖励黑客问题。ReTool使用简单的基于结果的奖励来缓解这一问题。Kevin-32B对响应施加严格的格式检查，并惩罚不期望的捷径（例如使用PyTorch回退）。DAPO的过长奖励调整机制防止了通过生成过长、可能正确但效率低下的响应来“刷分”。大型语言模型非常擅长发现奖励函数中的漏洞。如果奖励函数过于简单或没有考虑到所有不期望的行为，智能体将学会以非预期的方式最大化奖励信号，从而无法实现实际的任务目标。<br>设计鲁棒的奖励函数既是一门艺术也是一门科学。它通常需要根据观察到的失败模式进行迭代改进。趋势是朝着更细致的奖励组成部分（例如，2中结合任务奖励和格式奖励）和仔细考虑边缘情况发展，而不是仅仅依赖单一、简单的结果度量，特别是当任务变得更加开放式时。<br>下表2总结了不同研究中强化学习算法的实现及其关键特征。<br>强化学习的训练过程是一个精心设计的系统工程，涉及多个阶段和优化技术，旨在高效、稳定地提升智能体的策略。<br>典型的强化学习训练流程通常包含以下关键阶段：<br>•可选的监督微调(SupervisedFine-tuning,SFT)&#x2F;冷启动:一些框架选择在强化学习之前，首先在精心策划的数据集上进行监督微调。这为后续的强化学习阶段提供了一个鲁棒的初始化模型。例如，ReTool在代码增强数据集(DCI)上进行SFT，以教会模型何时以及如何调用代码解释器。DeepRetrieval在SQL数据库搜索任务中采用SFT作为冷启动策略。然而，也有研究采取不同的路径。TORL直接从基础语言模型开始进行强化学习，无需SFT阶段。VAPO则明确指出，为了保证与其他方法的公平比较，其在强化学习训练过程中不引入任何SFT数据。<br>•迭代式强化学习循环:这是强化学习的核心，通常包含以下子阶段的不断迭代：部署&#x2F;生成(Rollout&#x2F;Generation):策略模型根据当前的提示或状态生成行动序列（即轨迹）。<br>•评估&#x2F;奖励计算(Evaluation&#x2F;RewardCalculation):对生成的轨迹进行评估，并根据其与环境的交互结果或最终产出计算奖励。<br>•学习&#x2F;策略更新(Learning&#x2F;PolicyUpdate):基于获得的奖励和生成的轨迹，使用选定的强化学习算法（如PPO、GRPO）更新策略模型（以及价值模型，如果存在）。<br>为了确保训练过程的稳定性和效率，研究者们采用了多种优化技术：<br>•损失屏蔽(LossMasking):当外部工具的输出或检索到的信息作为输入序列的一部分时，这些外部词元通常在强化学习的损失计算中被屏蔽掉。这可以防止外部词元干扰策略梯度的优化，并确保训练的稳定性。<br>•KL散度正则化(KLDivergenceRegularization):这是一种常用的技术，通过惩罚当前策略与参考策略（通常是SFT模型或前一迭代的策略）之间的KL散度，来防止学习策略偏离过远，从而有助于维持训练稳定性。然而，在某些情况下，例如TORL和StarPO-S，为了增强探索，会有意省略KL惩罚项或将其系数设为0。<br>•梯度裁剪(GradientClipping):为了防止梯度爆炸导致训练不稳定，尤其是在处理大型模型或长序列时，有时会采用积极的梯度范数裁剪策略。<br>•动态采样&#x2F;轨迹过滤(DynamicSampling&#x2F;TrajectoryFiltering):DAPO框架中的“动态采样”技术会过滤掉那些所有生成输出的准确率均为0%或100%的提示，以确保训练批次中包含有效的梯度信息。StarPO-S则采用基于方差的轨迹过滤，保留具有高度不确定性的提示进行训练。<br>•预热阶段(Warm-upPhases):学习率预热或价值模型预热（如VAPO）有助于在训练初期稳定学习过程。<br>随着模型规模的增大和任务复杂度的提升，训练效率成为一个关键问题。<br>•规模化框架(FrameworksforScale):研究者们开发了如veRL和HybridFlow等专用框架，以支持大型语言模型的高效强化学习训练，这些框架通常内置了分布式训练能力。并行化(Parallelism):HybridFlow在训练期间使用张量并行，在推理期间使用混合数据-模型并行。<br>•KV缓存重用(KV-CacheReuse):ReTool在代码执行前缓存键值(KV)缓存，并且只计算和附加来自解释器反馈的KV缓存，以减少部署过程中的内存成本。异步操作(AsynchronousOperations):ReTool使用异步代码沙箱来加速强化学习训练过程。<br>•参数高效训练(Parameter-EfficientTraining):RAGEN框架探索了使用LoRA(Low-RankAdaptation)进行参数高效训练的方法。<br>关于初始化和技能获取的理念差异，体现在“先SFT后RL”与“直接RL”的路线选择上。ReTool和DeepRetrieval（针对SQL任务）明确将SFT作为“冷启动”或提供“鲁棒初始化”的手段。这种方法通过预先训练模型掌握期望的行为或工具交互格式，使得初始的RL探索阶段更具针对性和效率。然而，它也可能将模型偏向SFT数据的分布，从而潜在地限制RL阶段的探索广度。<br>相反，TORL倡导“直接从基础模型进行RL”而无需SFT，VAPO为了公平比较也避免在RL中使用SFT数据。在一个能力强大的基础模型上直接进行RL可能会发现更新颖的策略，但也可能面临更严峻的冷启动问题。这种选择可能取决于目标行为的复杂性、可用SFT数据的质量以及基础LLM的能力。目前，学术界仍在探索如何最好地结合监督学习和强化学习——无论是作为顺序过程、交错过程，还是主要将SFT模型用作参考策略。<br>大型语言模型强化学习的稳定性是一场多方面的战斗，需要通过算法调整、数据策略和过程管理的组合来解决。众多技术旨在稳定训练过程：KL正则化、PPO的裁剪机制（普遍使用）、解耦裁剪、价值预训练、动态采样&#x2F;过滤、外部词元损失屏蔽、梯度裁剪以及仔细的超参数调整。大型语言模型的训练本身就具有敏感性，而强化学习由于探索、稀疏奖励和潜在有噪声的价值估计，又增加了一层复杂性。如果没有这些稳定措施，训练很容易发散，导致策略崩溃或模型产生无意义的输出。<br>因此，实现大型语言模型强化学习的稳定性并非依赖单一的“银弹”，而是需要在整个训练流程中系统地解决潜在的故障点。这种整体方法对于使强化学习成为大型语言模型增强的可靠工具至关重要。专用框架（如veRL、HybridFlow）的出现也表明，需要专门设计基础设施来处理这些复杂性。<br>超参数是强化学习训练过程中的关键“旋钮”，它们的设置直接影响学习效率、稳定性和最终性能。<br>•学习率(Actor&amp;CriticLearningRates):通常设置得较小，例如行动家(actor)学习率为1×10−6，评论家(critic)学习率为1×10−5或2×10−6。如果使用评论家，行动家和评论家学习率的相对大小可能很重要。</p>
<p>•KL系数(β):控制策略偏离参考策略的惩罚程度。其值各不相同，例如ReTool中为0. 01，DeepRetrieval、SEARCH-R1、RAGEN中为0. 001，TORL中则省略。这一选择反映了在稳定性和探索之间的权衡。<br>•PPO裁剪参数(ϵ):标准值通常为0. 2。DAPO和VAPO使用解耦的ϵlow&#x3D;0. 2和ϵhigh&#x3D;0. 28。<br>•GAE参数(λ和γ):折扣因子γ通常对于非片段式任务或高度重视未来奖励的任务设置为1. 0。迹衰减参数λ对于PPO也通常设置为1. 0，但VAPO对策略网络使用长度自适应的λ，对价值网络使用λ&#x3D;1. 0。</p>
<p>•周期数&#x2F;训练步数(Epochs&#x2F;TrainingSteps):ReTool在冷启动数据上训练2个周期。SEARCH-R1训练500步。VAPO在AIME2024数据集上训练5000步达到领先水平。RAGEN使用200个部署-更新迭代。<br>尽管文献中并未总是明确详述超参数的调优策略，但不同研究中超参数设置的差异表明，实际调优通常基于具体的模型、数据集和任务进行经验性调整。学习率的预热计划是常见的做法。在训练过程中监控关键的中间结果，如生成的响应长度、奖励动态和模型熵，对于识别问题和指导调优至关重要。<br>超参数的选择往往反映了对特定任务和模型规模下探索-利用-稳定性三难困境的隐性理解。例如，将KL系数设置为0. 01或移除KL项，同时配合较高的生成温度，表明研究者有意推动更大程度的探索，这可能是因为任务复杂且初始策略远非最优。相反，当稳定性至关重要或策略已经相当不错时，可能会使用非零的KL系数和更保守的裁剪策略。“Clip-Higher”机制则是一种在不过多牺牲稳定性的前提下获得更多探索的精妙尝试。超参数直接控制学习动态。<br>激进的探索设置可能导致更快地发现新颖解决方案，但也存在策略崩溃的风险。保守的设置确保稳定性，但可能导致收敛缓慢或陷入局部最优。这表明可能不存在一套通用的“最佳”超参数组合，最优值高度依赖于具体情境。这也强调了对鲁棒超参数优化技术的需求，以及对每个超参数如何影响大型语言模型强化学习过程的深入理解。该领域可能会受益于对超参数敏感性和相互依赖性进行更系统的研究。<br>下表3展示了不同强化学习模型或研究中使用的超参数设置。<br>随着大型语言模型能力的增强，使其能够有效利用外部工具（如代码解释器、搜索引擎、数据库）和知识库，已成为强化学习研究的一个重要方向。这种集成旨在弥补大型语言模型在精确计算、实时信息获取以及与结构化数据交互等方面的不足。<br>当强化学习智能体需要学习与外部工具交互时，数据策略需要进行相应的调整和优化。<br>数据的选择首先由任务本身驱动，特别是那些天然需要或受益于工具使用的任务。<br>•对于数学推理任务，ReTool和TORL使用了数学竞赛题目，这类问题通常涉及复杂的计算，代码解释器可以作为有效的辅助工具。<br>•对于查询生成任务，DeepRetrieval采用了信息检索(IR)和SQL数据集，这些任务需要模型与搜索引擎或数据库进行交互。<br>•对于需要广泛背景知识或最新信息的网络研究任务，DeepResearcher使用了需要进行网页搜索和浏览的问答数据集。<br>•与通用强化学习类似，在工具增强型强化学习中，对工具使用结果的可验证性对于奖励函数的设定至关重要。<br>在工具集成场景下，数据清洗和过滤不仅要关注原始数据的质量，还需要考虑工具交互引入的复杂性。<br>•初始数据质量控制:与通用强化学习类似，初始数据集首先会经过清洗。例如，ReTool在将文本推理数据增强为代码集成数据之前，会先通过人工管理和模型评估来保证其质量。<br>•增强数据的验证:ReTool对其自动生成的代码集成数据(DCI)进行进一步验证，包括格式验证（确保工具调用触发器的正确性）和答案验证（确保最终输出与正确解一致）。这保证了用于训练模型学习工具使用的“增强数据”本身是高质量的。<br>•过滤以确保真实的工具需求:DeepResearcher的污染检测机制在此尤为关键。通过过滤掉那些模型无需搜索工具即可回答的问题，可以确保模型学习在真正需要时才使用搜索工具，而不是将其作为一种“万能膏药”。<br>为了让模型学会如何有效地使用工具，通常需要对数据进行特定形式的增强。<br>•自动构建工具集成数据:ReTool将基于文本的推理过程(Dinit)转换为代码集成的推理过程(DCI)是一个典型的数据增强策略。该过程通过用代码片段及其解释器执行结果替换原始推理过程中的手动计算步骤，从而为模型提供学习工具使用的“冷启动”数据。<br>对于工具增强型强化学习而言，数据管理往往涉及到创建期望工具交互模式的“范例”。ReTool自动构建代码集成数据的过程不仅仅是提供那些可以使用工具的问题，更重要的是主动展示了工具如何被整合到推理链条中。这些增强后的数据，尤其是在冷启动的监督微调阶段，充当了初始的监督样本。如果没有这样的范例，大型语言模型可能很难发现如何格式化工具调用、解析输出，甚至何时调用工具。增强数据通过提供具体的交互样例，有效地引导了这一学习过程。<br>这表明，对于复杂的工具使用场景，完全从零开始、仅依赖基于结果的强化学习可能效率极低。一种更为务实的途径是结合使用工具集成范例的监督学习和后续的强化学习微调。“数据”本身成为了指导工具交互协议的媒介。<br>将外部工具集成到强化学习循环中，需要在算法层面进行适应性调整，并设计合适的奖励机制。<br>尽管PPO和GRPO等标准算法仍然是核心，但为了适应工具交互，需要进行一些关键调整：<br>•用于工具调用的结构化输出:模型通常被训练成生成特定的词元或结构来触发工具的使用。例如，ReTool检测代码块结束标记来执行代码。SEARCH-R1使用和词元来调用搜索引擎。DeepRetrieval使用和标签，后者包含增强后的查询。DeepResearcher同样使用和标签，并将工具调用嵌入其中。RAGEN也采用了和的结构。<br>•解析工具输出:系统需要能够解析来自工具的输出（例如代码解释器的结果、搜索片段），并将其反馈到模型的上下文中。这通常通过特殊的标签来实现，如ReTool中的或SEARCH-R1中的。<br>奖励机制的设计对于引导模型有效使用工具至关重要。<br>•主要依赖最终结果:即使引入了工具，大多数系统仍然主要依赖任务的最终结果来提供奖励信号。如果工具的使用最终导致了问题的正确解决，那么这种工具使用行为就会得到正向强化。<br>•工具使用的隐性奖励:如果任务本身无法在不使用工具的情况下解决，那么对成功解决任务的奖励就隐性地包含了对成功使用工具的奖励。<br>•显式的工具相关奖励(较少见或效果不佳):TORL曾研究过为代码的可执行性提供奖励，但发现这并不能提升模型性能。ReTool也主要关注最终结果，而没有引入代码可执行性奖励。这表明，直接奖励工具使用的中间步骤（如代码是否可执行）可能难以设计，或者效果不如奖励最终结果。<br>•DeepRetrieval的奖励函数中包含了一项格式遵循奖励(rformat)，如果工具调用的语法包含在特定格式中，那么这项奖励可以间接支持正确的工具调用。<br>在工具增强型强化学习中，基于最终结果的奖励占据主导地位，这暗示了一种“结果导向”的策略，即依赖大型语言模型自身的推理能力来优化工具的使用方式。尽管工具交互过程可能非常复杂，但多数框架（如ReTool、TORL、SEARCH-R1）仍选择根据最终答案的正确性来给予奖励。尝试为中间步骤（如代码可执行性）添加显式奖励，效果并不总是理想。直接奖励工具使用的具体机制（例如，“代码是否成功运行？”）可能会导致智能体学会生成可运行但无用的代码。<br>通过关注最终结果，强化学习过程会迫使大型语言模型学习有效的工具使用——即那些有助于解决问题的工具使用。模型的内部推理被期望能够弥合工具调用与问题解决之间的鸿沟。这种做法对大型语言模型的推理能力以及强化学习算法在可能很长的工具交互链中恰当分配信用的能力提出了很高的要求。这也凸显了为复杂认知任务设计良好中间奖励的挑战性；通常情况下，稀疏的、基于结果的奖励虽然可能样本效率较低，但更为鲁棒。<br>当强化学习智能体需要与外部工具交互时，其训练过程具有一些独有的特点和挑战。<br>工具增强型强化学习的一个核心特征是模型生成部分推理，然后暂停以调用外部工具，接收工具的反馈，并基于该反馈继续进行后续的推理和生成。<br>•ReTool的流程是：大型语言模型生成文本，当检测到代码块结束标记时，生成的代码被发送到沙箱式代码解释器中执行。解释器的输出（成功结果或错误信息）随后被封装在标签内反馈给模型，模型再继续生成后续的推理轨迹。这种方式形成了一个文本、代码和解释器反馈交织的混合推理路径。<br>•TORL的模型会输出包含代码块的推理内容。当检测到代码终止标识符’’’output时，文本生成暂停，最新的代码块被提取出来交由代码解释器（如SandboxFusion）执行。结构化的执行结果(OBSERVATION)会被插回上下文中，模型随后继续生成后续的自然语言推理，并可能产生更多的代码块，直至给出最终答案。<br>•SEARCH-R1的模型在生成文本时，如果产生特定的词元，系统会提取查询内容，调用搜索引擎，并将检索到的结果通过标签注入回模型的上下文中，供模型进行后续的推理和答案生成。<br>•DeepResearcher的智能体首先在标签内进行推理，然后根据需要调用网页搜索或网页浏览工具。从这些工具获得的观察结果会更新智能体的短期记忆，辅助后续决策。<br>如何处理来自外部工具的反馈，尤其是错误信息，是训练过程中的一个重要环节。<br>•错误信息作为学习信号:来自工具执行的错误信息（例如代码编译错误或运行时错误）通常会被刻意地返回给大型语言模型。这有助于模型学习生成语法正确且语义合理的工具输入。例如，TORL明确指出会将代码执行失败的错误信息返回给模型，以增强其后续生成正确代码的能力。ReTool的描述中也提到沙箱会返回错误信息，暗示了类似机制。<br>•屏蔽工具输出以避免干扰损失计算:正如在核心强化学习方法论中讨论过的（章节II. C.2），来自工具的实际内容输出（例如代码解释器的执行结果、搜索引擎返回的文本片段）通常在强化学习的损失计算中被屏蔽掉。这样做是为了确保模型学习的是利用这些信息进行推理，而不是简单地模仿或复制这些外部信息。同时，这也有助于维持训练的稳定性，防止外部引入的、可能与模型自身生成逻辑不一致的词元干扰策略梯度的计算。<br>当集成的外部工具具有执行任意代码或与外部世界进行不受控交互的能力时，安全问题就变得至关重要。<br>•代码在沙箱中执行:对于像代码解释器这样的工具，其执行过程通常被置于一个沙箱环境中。沙箱提供了一个隔离的环境，用于执行由大型语言模型生成的代码，从而确保安全性和可控性，防止潜在的恶意代码或意外操作对系统造成损害。TORL选择了SandboxFusion作为其代码执行环境，因为它具有较好的稳定性。<br>•异步沙箱提升效率:为了加速训练过程，特别是在需要频繁与代码解释器等工具交互的场景下，ReTool设计了一个异步代码沙箱环境。该环境中的沙箱实例作为工作池中的工作者，可以独立地拉取任务并执行，从而形成高效的负载均衡机制，并支持并行的环境交互。<br>无限制的工具调用可能会导致训练效率低下或产生冗余的交互。因此，需要机制来控制工具的使用频率。<br>•最大工具调用次数限制:TORL框架引入了一个超参数C，用于控制在一次响应生成过程中允许的最大工具调用次数。如果超过此阈值，后续的工具执行请求将被忽略，迫使模型切换到纯文本推理模式。这有助于在保证一定探索深度的同时维持训练速度。SEARCH-R1也使用了一个最大行动预算B来限制搜索次数。DeepResearcher允许每个部署轨迹最多进行10次工具调用。<br>对工具输出进行“损失屏蔽”是一项关键技术，其目的是迫使大型语言模型学习“如何利用工具进行思考”，而不是仅仅学习“工具会输出什么”。多项研究1都明确提到了在强化学习损失计算过程中，屏蔽掉来自工具输出的词元（例如代码解释器的结果、搜索片段）。如果这些外部词元被包含在策略更新的损失计算中，大型语言模型可能会学会简单地预测或复制这些词元，特别是当它们冗长或包含强信号时。这将绕过预期的学习目标，即让模型理解并利用工具提供的信息来指导其自身的后续推理。<br>通过屏蔽，梯度只流经模型自身生成的词元，从而强化其推理和决策能力（例如，在给定工具输出后决定下一步做什么）。这突显了训练大型语言模型使用工具时一个微妙但至关重要的方面：区分整合信息与仅仅复述信息。有效的工具使用要求大型语言模型充当外部信息的智能消费者和整合者，而训练过程必须精心设计以培养这种能力。<br>工具增强型强化学习中“生成-执行-反馈-再生成”的迭代循环，在某种程度上反映了人类解决问题的过程，但它也要求对状态和上下文进行细致的管理。ReTool、TORL、SEARCH-R1和DeepResearcher的描述都详细说明了这样一个过程：大型语言模型生成一些推理或工具查询，外部工具执行该查询，然后结果被反馈到模型的上下文中，用于下一步的生成。这种迭代过程使得大型语言模型能够分解复杂问题，逐步收集信息或执行计算，并根据中间结果调整其策略。<br>然而，这也带来了挑战：上下文窗口可能会变得非常大，状态表示需要有效地整合不同类型的反馈（文本、数字、错误），并且在长的多步交互中进行信用分配变得更加困难。这种范式对于解决复杂的多步骤任务非常强大。然而，其成功取决于高效的上下文管理（例如ReTool的KV缓存重用，Kevin-32B的思维链总结）、鲁棒的错误处理，以及能够在这些扩展交互中从延迟奖励中学习的强化学习算法。开发混合自然语言和工具交互的“推理轨迹”或“交互轨迹”是一个关键的研究方向。<br>在工具集成型强化学习中，除了通用的强化学习超参数外，还需要考虑一些与工具交互特性相关的特定超参数。</p>
<p>•检索内容&#x2F;工具输出的最大长度(MaximumLengthforRetrievedContent&#x2F;ToolOutput):SEARCH-R1为检索到的内容设置了500个词元的最大长度。这会影响反馈给模型的信息量，并进而影响上下文窗口的管理和模型的注意力分配。<br>核心的强化学习超参数（如学习率、批量大小等）在工具集成场景下依然至关重要。然而，由于工具交互改变了学习动态（例如，如果成功的工具使用过程复杂，奖励可能变得更稀疏；或者轨迹长度可能发生变化），这些超参数的最优值可能会发生偏移。<br>文献中并未总是明确区分工具集成型强化学习与非工具型强化学习的超参数设置。但总体而言，工具交互的引入可能会增加学习任务的复杂性，因此可能需要更仔细的调优，或倾向于选择更鲁棒、更稳定的设置。例如，ReTool在其工具集成框架中将KL系数设置为0. 01，这可能是为了鼓励模型在学习工具使用模式时进行更广泛的探索。<br>控制工具交互“粒度”和“体量”的超参数（如最大调用次数、最大输出长度）对于平衡学习效果与计算约束至关重要。诸如最大工具调用次数和检索内容最大长度等参数，直接影响强化学习智能体探索和学习的轨迹复杂性。<br>更多的工具调用或更长的输出可以提供更多信息，但也增加了序列长度、每步的计算成本，并可能增加学习信号中的噪声。如果最大工具调用次数设置过低，智能体可能无法解决复杂的多步骤问题。如果设置过高，训练可能会变得非常缓慢，或者智能体可能学会低效、冗长的策略。<br>类似地，过长的工具输出可能会超出上下文窗口的限制或稀释重要的信号。因此，优化这些工具特定的超参数对于实用的工具增强型强化学习至关重要。这是一个在赋予智能体足够自由度以学习复杂交互与保持训练过程易于管理和专注之间的权衡。这可能会推动自适应策略的发展，即这些限制在训练过程中动态变化。<br>下表4总结了在不同研究中，强化学习训练所集成的外部工具或知识库。<br>通过对上述强化学习训练方法论的深入分析，我们可以观察到一些趋同的主题和差异化的策略，识别新兴趋势与挑战，并据此提出一些最佳实践建议和未来研究方向。<br>在众多强化学习应用于大型语言模型的研究中，出现了一些共同的趋势和方法选择：<br>•PPO&#x2F;GRPO的主导地位:PPO及其变种GRPO已成为强化学习训练大型语言模型的事实标准算法，这得益于它们在稳定性与实现相对简单性之间的平衡。<br>•结果导向的奖励函数:尽管任务各异，但大多数研究倾向于使用基于最终任务结果（如答案正确性、任务完成度）的奖励函数，这种方式简单直接，且在一定程度上可以避免对中间过程的过度拟合。<br>•高质量、精细化数据的关键性:各项研究普遍强调高质量、经过精心筛选和过滤的数据对于成功训练的重要性，包括去除噪声、确保相关性以及防止数据污染。<br>•外部信息损失屏蔽的普遍性:在集成外部工具或知识库时，普遍采用将外部信息（如工具输出、检索内容）在损失计算中屏蔽的做法，以确保模型学习的是利用信息进行推理，而非简单模仿。<br>•SFT与直接RL的选择:对于模型初始化，一些研究采用先进行监督微调(SFT)再进行强化学习的策略，而另一些则选择直接从基础模型开始进行强化学习，这反映了对初始化效率与探索自由度之间不同权衡的考量。<br>•有无评论家的优势估计:PPO通常依赖于学习的价值网络（评论家）来估计优势函数，而GRPO等方法则通过组内奖励归一化等方式避免了评论家，这在计算开销和估计精度之间形成了取舍。<br>•探索与稳定性的具体技术:尽管目标一致，但不同研究在如何平衡探索与稳定性方面采用了不同的具体技术，例如DAPO和VAPO中引入的Clip-Higher、长度自适应GAE等。<br>•奖励函数的复杂程度:奖励函数的设计从简单的二元奖励到包含格式遵循、效率考量等多个组成部分的复合奖励，其复杂程度因任务和研究目标而异。<br>强化学习在大型语言模型领域的应用正呈现出一些积极的趋势，但也面临着持续的挑战：<br>•算法日益复杂化与定制化:针对大型语言模型的特性和特定任务的需求，研究者们正在开发越来越复杂和定制化的强化学习算法，如VAPO、DAPO、Dr. GRPO和StarPO等，它们在经典算法基础上进行了诸多创新。<br>•外部工具集成的多样化与深度化:模型集成的外部工具类型日益丰富，从最初的计算器、代码解释器，扩展到搜索引擎、数据库接口、乃至复杂的网络浏览和专业开发环境。<br>•关注多轮交互与轨迹级优化:随着任务复杂性的增加，对模型进行多轮交互和整个交互轨迹层面的优化受到更多关注，如StarPO和Kevin-32B的研究所示。<br>•数据中心方法的强化:对数据在强化学习中作用的认识不断深化，出现了如污染过滤、策略性数据增强等更为精细的数据处理方法。<br>•样本效率:尤其对于奖励稀疏或交互成本高的复杂任务，提升强化学习的样本效率仍然是一个核心挑战。<br>•长程信用分配:在涉及多步推理和工具使用的长交互轨迹中，如何准确地将最终奖励分配给序列中的关键决策是一个难题。<br>•训练的可扩展性:随着模型规模的增大和工具交互复杂性的提升，如何高效、可扩展地进行强化学习训练是一个持续的工程挑战。<br>•泛化能力:如何确保模型学习到的工具使用策略或推理模式能够泛化到新的工具、任务或未见过的数据分布上，是衡量其真正能力的关键。<br>•奖励黑客与真实理解:设计能够有效避免奖励黑客行为、并能真正反映模型理解能力的奖励函数，仍然是一个开放性问题。<br>基于当前的认知，可以为设计强化学习训练流程提供以下一般性建议：<br>•数据为王:从与目标技能高度相关的高质量、经过精心管理和过滤的数据开始。考虑数据的多样性、难度分布以及潜在的污染问题。<br>•SFT引导:如果有高质量的监督微调数据可用，可以考虑使用SFT来引导模型学习复杂行为或工具交互的基本格式，这有助于加速后续强化学习的收敛。<br>•算法选择与适配:选择一个成熟的强化学习算法家族（如PPO&#x2F;GRPO），并根据计算预算、稳定性需求以及任务特性进行适配。例如，在计算资源受限时可以考虑无评论家方法，而在追求更高性能时可以探索如VAPO等更先进的价值估计算法。<br>•奖励设计:奖励函数的设计应尽可能简单明了，同时要能有效抵抗奖励黑客行为。优先考虑基于最终任务结果的奖励。<br>•损失屏蔽:对于任何集成到模型上下文中的外部信息（如工具输出），务必在强化学习的损失计算中进行屏蔽。<br>•稳定训练:采用多种技术来确保训练过程的稳定性，包括但不限于KL散度正则化、梯度裁剪、以及细致的超参数调优。<br>•工具使用规范:在集成外部工具时，确保在安全的环境（如沙箱）中执行，并将工具的错误反馈作为学习信号提供给模型。同时，通过设置最大调用次数等机制来控制工具的交互频率。<br>•迭代监控与优化:强化学习的训练是一个迭代的过程。需要持续监控训练动态（如奖励曲线、生成内容质量、模型熵等），并根据观察结果不断调整数据、奖励函数和超参数。<br>展望未来，强化学习在大型语言模型领域的应用仍有广阔的探索空间：<br>•更高样本效率的算法:探索利用基于模型的强化学习、离线强化学习等技术，以进一步提升样本效率。<br>•分层强化学习:对于需要处理复杂、多层次任务和工具使用场景，分层强化学习可能提供更有效的解决方案。<br>•自动化奖励设计:研究如何自动设计或学习有效的奖励函数，以减轻人工设计奖励的负担和偏见。<br>•长轨迹信用分配的改进:开发更先进的信用分配方法，以解决在长交互序列中学习的挑战。<br>•标准化基准与环境:建立针对工具增强型强化学习的标准化基准测试和模拟环境，以促进不同方法之间的公平比较和可复现研究。<br>•参数化知识与外部信息的深度融合:更深入地研究大型语言模型如何在其参数化知识与通过工具获取的外部信息之间进行有效的权衡、整合与推理。<br>本文对强化学习的训练方法进行了系统性的梳理与分析。核心发现包括：数据管理在强化学习中扮演着基础性且日益重要的角色，精细化的数据选择、清洗、过滤和增强策略是成功训练的关键；<br>以PPO和GRPO为代表的策略优化算法是当前的主流选择，同时针对大型语言模型的特性和特定任务需求，涌现出如DAPO、VAPO、Dr. GRPO、StarPO等一系列创新算法和技术；训练过程通常涉及可选的监督微调、迭代式的强化学习循环，并广泛采用损失屏蔽、KL正则化等稳定性措施；超参数的精细调整对于平衡探索、利用与稳定性至关重要。<br>特别地，将强化学习与外部工具和知识库集成已成为提升大型语言模型能力的重要途径。为此，研究者们发展了针对性的数据增强方法（如自动构建工具交互范例）、在算法层面支持结构化工具调用与反馈解析、在训练过程中实现推理与工具执行的交错循环、通过沙箱环境确保安全，并通过损失屏蔽和错误反馈等机制引导模型学习有效的工具使用策略。<br>强化学习在大型语言模型领域的应用正从早期对通用算法的直接套用，迅速发展为针对模型特性和任务需求的高度专业化技术。数据策略的精细化、算法的持续创新以及训练过程的系统化管理，三者之间的协同作用是解锁大型语言模型在复杂推理和工具使用方面强大潜力的关键。<br>未来，该领域的进展将可能继续依赖于在这些方面的持续突破，特别是在提升样本效率、增强算法的可扩展性、以及使模型能够从日益复杂的交互和反馈中学习等方面。随着研究的不断深入，我们有理由相信，强化学习将为构建更智能、更通用的人工智能系统贡献核心力量。<br>进技术交流群请添加AINLP小助手微信（id:ainlp2)<br>请备注具体方向+所用到的相关技术点<br>关于AINLP<br>AINLP是一个有趣有AI的自然语言处理社区，专注于AI、NLP、机器学习、深度学习、推荐算法等相关技术的分享，主题包括LLM、预训练模型、自动生成、文本摘要、智能问答、聊天机器人、机器翻译、知识图谱、推荐系统、计算广告、招聘信息、求职经验分享等，欢迎关注！加技术交流群请添加AINLP小助手微信(id：ainlp2)，备注工作&#x2F;研究方向+加群目的。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">ZejunCao</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://zejuncao.github.io/2025/05/26/1000002318-2650449039-2/">https://zejuncao.github.io/2025/05/26/1000002318-2650449039-2/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">ZejunCao</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/AINLP/">
                                    <span class="chip bg-color">AINLP</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/05/26/1000002318-2650449039-1/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/1000002318_2650449039_1.jpg" class="responsive-img" alt="开源免费大模型教材.pdf">
                        
                        <span class="card-title">开源免费大模型教材.pdf</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-05-26
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AINLP/">
                        <span class="chip bg-color">AINLP</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2025/05/26/1000002318-2650449039-3/">
                    <div class="card-image">
                        
                        <img src="/medias/frontcover/1000002318_2650449039_3.jpg" class="responsive-img" alt="Adaptive Reasoning Model：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaThinking">
                        
                        <span class="card-title">Adaptive Reasoning Model：Qwen3混合思考-&gt;字节AdaCoT-&gt;清华AdaThinking</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2025-05-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            ZejunCao
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/AINLP/">
                        <span class="chip bg-color">AINLP</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2025</span>
            
            <a href="/about" target="_blank">ZejunCao</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ZejunCao" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:caozejun369@163.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1378463428" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1378463428" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="https://weibo.com/u/5915009280" class="tooltipped" target="_blank" data-tooltip="关注我的微博: https://weibo.com/u/5915009280" data-position="top" data-delay="50">
        <i class="fab fa-weibo"></i>
    </a>



    <a href="https://www.zhihu.com/people/Garfusion/posts" class="tooltipped" target="_blank" data-tooltip="关注我的知乎: https://www.zhihu.com/people/Garfusion/posts" data-position="top" data-delay="50">
        <i class="fab fa-zhihu1">知</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
